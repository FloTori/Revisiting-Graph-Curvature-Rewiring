{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCkJMjtW-i9o",
        "outputId": "89859d2b-9eb8-431d-9677-3af93c319091"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "\n",
        "from scipy import sparse\n",
        "import math\n",
        "from numba import cuda\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"using device: \", device)\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n",
        "import json\n",
        "\n",
        "import random as random\n",
        "from math import ceil\n",
        "\n",
        "from utils.seeds import val_seeds\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nRoKGCof-i9q"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import KarateClub,Planetoid\n",
        "\n",
        "from torch_geometric.datasets import WebKB #Cornell,Texas & Wisconsin dataset\n",
        "\n",
        "from torch_geometric.datasets import Actor\n",
        "\n",
        "from torch_geometric.datasets import WikipediaNetwork #Squirrel & Chameleon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auSq9JTM-i9s"
      },
      "source": [
        "## Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adZSSFZS-i9t"
      },
      "outputs": [],
      "source": [
        "development_seed = 1684992425"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZvEuhjB-i9t"
      },
      "source": [
        "## Dictionaries parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIL2r04s-i9u"
      },
      "outputs": [],
      "source": [
        "hyperparameters_Neurips_1 = {\n",
        "    \"Cora\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"max\": 0.5555, \"min\": 0.0001},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"max\": 0.9999,\"min\":0.0001},\n",
        "            \"weight_decay\":{\"max\": 0.5555, \"min\": 0.0001},\n",
        "\t\t\t\"loops\": {\"min\":80,\"max\":120},\n",
        "\t\t\t\"tau\":{\"min\": 25, \"max\": 216},\n",
        "\t\t\t\"C+\": {\"min\": 0.2, \"max\": 17.5}\n",
        "\n",
        "        }\n",
        "    },\n",
        "    \"Citeseer\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"max\": 0.5555, \"min\": 0.0001},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"max\": 0.9999,\"min\":0.0001},\n",
        "            \"weight_decay\":{\"max\": 0.5555, \"min\": 0.0001},\n",
        "\t\t\t\"loops\": {\"min\":67,\"max\":101},\n",
        "\t\t\t\"tau\":{\"min\": 25, \"max\": 216},\n",
        "\t\t\t\"C+\": {\"min\": 0.2, \"max\": 17.5}\n",
        "\n",
        "        }\n",
        "\t},\n",
        "    \"Pubmed\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"max\": 0.5555, \"min\": 0.0001},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"max\": 0.9999,\"min\":0.0001},\n",
        "            \"weight_decay\":{\"max\": 0.5555, \"min\": 0.0001},\n",
        "\t\t\t\"loops\": {\"min\":92,\"max\":138},\n",
        "\t\t\t\"tau\":{\"min\": 25, \"max\": 216},\n",
        "\t\t\t\"C+\": {\"min\": 0.2, \"max\": 17.5}\n",
        "        }\n",
        "\t},\n",
        "    \"Cornell\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"min\": 0.0001, \"max\": 0.5555},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"min\":0.0001,\"max\": 0.9999},\n",
        "            \"weight_decay\":{\"min\": 0.0001, \"max\": 0.5555},\n",
        "\t\t\t\"loops\": {\"min\":100,\"max\":151},\n",
        "\t\t\t\"tau\":{\"min\": 9, \"max\": 302},\n",
        "\t\t\t\"C+\": {\"min\": 0.7, \"max\": 21.2}\n",
        "        }\n",
        "\t},\n",
        "    \"Texas\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"min\": 0.0001, \"max\": 0.5555},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"min\":0.0001,\"max\": 0.9999},\n",
        "            \"weight_decay\":{\"min\": 0.0001, \"max\": 0.5555},\n",
        "\t\t\t\"loops\": {\"min\":71,\"max\":107},\n",
        "\t\t\t\"tau\":{\"min\": 9, \"max\": 302},\n",
        "\t\t\t\"C+\": {\"min\": 0.7, \"max\": 21.2}\n",
        "        }\n",
        "\t},\n",
        "    \"Wisconsin\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"min\": 0.0001, \"max\": 0.5555},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"min\":0.0001,\"max\": 0.9999},\n",
        "            \"weight_decay\":{\"min\": 0.0001, \"max\": 0.5555},\n",
        "\t\t\t\"loops\": {\"min\":108,\"max\":163},\n",
        "\t\t\t\"tau\":{\"min\": 9, \"max\": 302},\n",
        "\t\t\t\"C+\": {\"min\": 0.7, \"max\": 21.2}\n",
        "        }\n",
        "\t},\n",
        "    \"Chameleon\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"min\": 0.0001, \"max\": 0.5555},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"min\":0.0001,\"max\": 0.9999},\n",
        "            \"weight_decay\":{\"min\": 0.0001, \"max\": 0.5555},\n",
        "\t\t\t\"loops\": {\"min\":665,\"max\":999},\n",
        "\t\t\t\"tau\":{\"min\": 9, \"max\": 302},\n",
        "\t\t\t\"C+\": {\"min\": 0.7, \"max\": 21.2}\n",
        "        }\n",
        "\t},\n",
        "    \"Squirrel\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"min\": 0.0001, \"max\": 0.5555},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"min\":0.0001,\"max\": 0.9999},\n",
        "            \"weight_decay\":{\"min\": 0.0001, \"max\": 0.5555},\n",
        "\t\t\t\"loops\": {\"min\":4925,\"max\":7325},\n",
        "\t\t\t\"tau\":{\"min\": 9, \"max\": 302},\n",
        "\t\t\t\"C+\": {\"min\": 0.7, \"max\": 21.2}\n",
        "        }\n",
        "\t},\n",
        "    \"Actor\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"min\": 0.0001, \"max\": 0.5555},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"min\":0.0001,\"max\": 0.9999},\n",
        "            \"weight_decay\":{\"min\": 0.0001, \"max\": 0.5555},\n",
        "\t\t\t\"loops\": {\"min\":808,\"max\":1212},\n",
        "\t\t\t\"tau\":{\"min\": 9, \"max\": 302},\n",
        "\t\t\t\"C+\": {\"min\": 0.7, \"max\": 21.2}\n",
        "        }\n",
        "\t}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHhTrhKX-i9u"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxqyiq14-i9v"
      },
      "source": [
        "## Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwBXkNv2-i9v"
      },
      "outputs": [],
      "source": [
        "DEFAULT_DATA_PATH = \"data\"\n",
        "\n",
        "def get_dataset(\n",
        "    name: str, data_dir=DEFAULT_DATA_PATH\n",
        "):\n",
        "    #path = os.path.join(data_dir, name)\n",
        "    path = DEFAULT_DATA_PATH\n",
        "    if name in [\"Cora\", \"Citeseer\", \"Pubmed\"]:\n",
        "        dataset = Planetoid(path, name)\n",
        "    elif name in [\"Computers\", \"Photo\"]:\n",
        "        dataset = Amazon(path, name)\n",
        "    elif name == \"CoauthorCS\":\n",
        "        dataset = Coauthor(path, \"CS\")\n",
        "    elif name in [\"Cornell\", \"Texas\", \"Wisconsin\"]:\n",
        "        dataset = WebKB(path, name)\n",
        "    elif name in [\"Chameleon\", \"Squirrel\"]:\n",
        "        dataset = WikipediaNetwork(path, name, geom_gcn_preprocess=True)\n",
        "    elif name == \"Actor\":\n",
        "        dataset = Actor(path, \"Actor\")\n",
        "    else:\n",
        "        raise Exception(f\"Unknown dataset: {name}\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data(name: str, make_undirected: bool = False):\n",
        "    dataset = get_dataset(name)\n",
        "    data = dataset[0]\n",
        "    G = torch_geometric.utils.to_networkx(data)\n",
        "    if data.is_undirected() or make_undirected:#undirected:\n",
        "        G = G.to_undirected() #This is for Networkx to represent it as a undirected Graph (Otherwise it would 'plot' i->j and j->i as two different edges)\n",
        "\n",
        "    return dataset,data,G\n",
        "\n",
        "def data_information(dataset,data):\n",
        "    print()\n",
        "    print(f'Dataset: {dataset}:')\n",
        "    print('======================')\n",
        "\n",
        "    print(f'Number of features: {dataset.num_features}')\n",
        "    print(f'Number of classes: {dataset.num_classes}')\n",
        "    print()\n",
        "\n",
        "    # Gather some statistics about the graph.\n",
        "    print(f'Number of nodes: {data.num_nodes}')\n",
        "    print(f'Number of edges: {data.num_edges}')\n",
        "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "    print(f'Has self-loops: {data.has_self_loops()}')\n",
        "    print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZgcU6fZ-i9x"
      },
      "source": [
        "## LargestConnectedCommonent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVt_DHRc-i9x"
      },
      "outputs": [],
      "source": [
        "def get_largest_connected_component_pytorch(connectiontype: str, directed ,data,num_nodes:int):\n",
        "    \"\"\"\n",
        "    Pytorch (backend Scipy) implementation of lcc calculation\n",
        "\n",
        "    Args:\n",
        "    edge_index (torch tensor): edge index of the graph\n",
        "    num nodes (int): number of nodes of the graph\n",
        "    directed: (boolean): Wether input graph is directed or not.  If directed == False, connectiontype keyword is not referenced.\n",
        "    connectiontype (str): Only relevant for directed graphs. Weak or Strong constrain for largest connected component\n",
        "\n",
        "    Returns:\n",
        "    Subgraph (Networkx Graph): Subgraph which corresponds to the largest connected component of the graph\n",
        "    \"\"\"\n",
        "\n",
        "    adj = torch_geometric.utils.to_scipy_sparse_matrix(data.edge_index, num_nodes=num_nodes)\n",
        "    num_components, component = sparse.csgraph.connected_components(adj, directed=directed, connection=connectiontype)\n",
        "    #print(\"Total Number of Components: \",num_components)\n",
        "\n",
        "    _, count = np.unique(component, return_counts=True)\n",
        "    subset_np = np.in1d(component, count.argsort()[-1:])\n",
        "    subset = torch.from_numpy(subset_np)\n",
        "    subset = subset.to(data.edge_index.device, torch.bool)\n",
        "    Subgraph = torch_geometric.utils.to_networkx(data.subgraph(subset))\n",
        "    #print(\"Largest Connected Component size: \", len(Subgraph.nodes))\n",
        "    return data.subgraph(subset)#Subgraph\n",
        "\n",
        "def get_largest_connected_component_networkx(connectiontype: str,directed, G):\n",
        "    \"\"\"\n",
        "    Network implementation of lcc calculation\n",
        "\n",
        "    Args:\n",
        "    G (networkx graph): Input Graph\n",
        "    connectiontype (str or None): Only relevant for directed graphs. Weak or Strong constrain for largest connected component\n",
        "\n",
        "    Returns:\n",
        "    Subgraph (Networkx Graph): Subgraph which corresponds to the largest connected component of the graph\n",
        "    \"\"\"\n",
        "    if not directed:\n",
        "        return G.subgraph(max(nx.connected_components(G), key=len)).copy()\n",
        "    elif directed and connectiontype == 'strong':\n",
        "        return G.subgraph(max(nx.strongly_connected_components(G), key=len)).copy()\n",
        "    elif directed and connectiontype == 'weak':\n",
        "        return G.subgraph(max(nx.weakly_connected_components(G), key=len)).copy()\n",
        "\n",
        "def get_component_toppingetal(data, start: int = 0) -> set:\n",
        "    visited_nodes = set()\n",
        "    queued_nodes = set([start])\n",
        "    row, col = data.edge_index.numpy()\n",
        "    while queued_nodes:\n",
        "        current_node = queued_nodes.pop()\n",
        "        visited_nodes.update([current_node])\n",
        "        neighbors = col[np.where(row == current_node)[0]]\n",
        "        neighbors = [\n",
        "            n for n in neighbors if n not in visited_nodes and n not in queued_nodes\n",
        "        ]\n",
        "        queued_nodes.update(neighbors)\n",
        "    return visited_nodes\n",
        "\n",
        "def get_largest_connected_component_toppingetal(data):\n",
        "\n",
        "    remaining_nodes = set(range(data.x.shape[0]))\n",
        "    comps = []\n",
        "    while remaining_nodes:\n",
        "        start = min(remaining_nodes)\n",
        "        comp = get_component_toppingetal(data, start)\n",
        "        comps.append(comp)\n",
        "        remaining_nodes = remaining_nodes.difference(comp)\n",
        "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
        "\n",
        "def remap_edges(edges: list, mapper: dict) -> list:\n",
        "    row = [e[0] for e in edges]\n",
        "    col = [e[1] for e in edges]\n",
        "    row = list(map(lambda x: mapper[x], row))\n",
        "    col = list(map(lambda x: mapper[x], col))\n",
        "    return [row, col]\n",
        "\n",
        "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
        "    mapper = {}\n",
        "    counter = 0\n",
        "    for node in lcc:\n",
        "        mapper[node] = counter\n",
        "        counter += 1\n",
        "    return mapper\n",
        "\n",
        "def lcc_dataset(dataset,to_undirected = True):\n",
        "    lcc = get_largest_connected_component_toppingetal(dataset[0])\n",
        "\n",
        "    x_new = dataset.data.x[lcc]\n",
        "    y_new = dataset.data.y[lcc]\n",
        "\n",
        "    row, col = dataset.data.edge_index.numpy()\n",
        "    edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
        "    edges = remap_edges(edges, get_node_mapper(lcc))\n",
        "\n",
        "    if to_undirected:\n",
        "        data = Data(\n",
        "            x=x_new,\n",
        "            edge_index=torch_geometric.utils.to_undirected(torch.LongTensor(edges)),\n",
        "            y=y_new,\n",
        "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "             )\n",
        "    else:\n",
        "        data = Data(\n",
        "            x=x_new,\n",
        "            edge_index=torch.LongTensor(edges),\n",
        "            y=y_new,\n",
        "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "             )\n",
        "    dataset.data = data\n",
        "\n",
        "    mapping = dict(\n",
        "        zip(np.unique(dataset.data.y), range(len(np.unique(dataset.data.y))))\n",
        "    )\n",
        "    dataset.data.y = torch.LongTensor([mapping[u] for u in np.array(dataset.data.y)])\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H5t3NEQ-3rB"
      },
      "source": [
        "## GNN Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1U2xsxS-7GB"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from torch.nn import ModuleList, Dropout, ReLU\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class GCN_Toppingetal(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, dataset, hidden: List[int] = [64], dropout: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        num_features = [dataset.data.x.shape[1]] + hidden + [dataset.num_classes]\n",
        "        layers = []\n",
        "        for in_features, out_features in zip(num_features[:-1], num_features[1:]):\n",
        "            layers.append(GCNConv(in_features, out_features))\n",
        "        self.layers = ModuleList(layers)\n",
        "\n",
        "        self.reg_params = list(layers[0].parameters())\n",
        "        self.non_reg_params = list([p for l in layers[1:] for p in l.parameters()])\n",
        "\n",
        "        self.dropout = Dropout(p=dropout)\n",
        "        self.act_fn = ReLU()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_parameters()\n",
        "\n",
        "    def forward(self, data: Data,device):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        #x = x.to(device = device)\n",
        "        #edge_index = edge_index.to(device = device)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x, edge_index, edge_weight=edge_attr)\n",
        "\n",
        "            if i == len(self.layers) - 1:\n",
        "                break\n",
        "\n",
        "            x = self.act_fn(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        return torch.nn.functional.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features,num_classes,hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(num_features,hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels,num_classes)\n",
        "    def forward(self, data):\n",
        "        x,edge_index = data.x,data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.4144)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux0jht5T--DJ"
      },
      "source": [
        "## Experiment Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH1WSXsY--3B"
      },
      "outputs": [],
      "source": [
        "def load_hyperparameters_gnn(dataset_name):\n",
        "    try:\n",
        "        with open(os.path.join('experiment_utils','hyperparameters_gnn.json'), 'r') as file:\n",
        "            hyperparameters_data = json.load(file)\n",
        "            return hyperparameters_data.get(dataset_name, {})\n",
        "    except FileNotFoundError:\n",
        "        print(\"Hyperparameters file not found.\")\n",
        "        return {}\n",
        "\n",
        "class Experiment():\n",
        "    def __init__(self,device,datasetname,dataset,data,hyperparameters):\n",
        "\n",
        "        #self.model = GCN(dataset.num_features,dataset.num_classes,hidden_channels=64 )\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "        self.lr = hyperparameters[\"learning_rate\"]\n",
        "        self.layers = hyperparameters[\"layers\"]\n",
        "        self.weight_decay = hyperparameters[\"weight_decay\"]\n",
        "        self.dropout = hyperparameters[\"dropout\"]\n",
        "\n",
        "        self.model = GCN_Toppingetal(dataset,self.layers,self.dropout).to(device = device)\n",
        "        self.device = device\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay= self.weight_decay)\n",
        "\n",
        "        self.epoch = 10000#hyperparameters[\"epochs\"]\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "        out = self.model(self.data,self.device)  # Perform a single forward pass.\n",
        "        loss = F.nll_loss(out[self.data.train_mask], self.data.y[self.data.train_mask].to(self.device))  # Compute the loss solely based on the training nodes.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        self.optimizer.step()  # Update parameters based on gradients.\n",
        "        return loss\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        out = self.model(self.data,self.device)  # Perform a single forward pass.\n",
        "        pred = out[self.data.val_mask].max(1)[1]\n",
        "        val_acc = pred.eq(self.data.y[self.data.val_mask]).sum().item() / self.data.val_mask.sum().item()\n",
        "\n",
        "        return val_acc\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        out = self.model(self.data,self.device)  # Perform a single forward pass.\n",
        "        pred = out[self.data.test_mask].max(1)[1]\n",
        "        test_acc = pred.eq(self.data.y[self.data.test_mask]).sum().item() / self.data.test_mask.sum().item()\n",
        "\n",
        "        return test_acc\n",
        "\n",
        "    def training(self):\n",
        "        losses = []\n",
        "        validations = []\n",
        "        counter = 0\n",
        "        for epoch in range(1, self.epoch):\n",
        "            loss = self.train()\n",
        "            losses.append(loss.detach().cpu().numpy())\n",
        "            val = self.validate()\n",
        "            validations.append(val)\n",
        "            if epoch ==1:\n",
        "                best_val = val\n",
        "            elif epoch > 1 and val >= best_val:\n",
        "                best_val = val\n",
        "                counter = 0\n",
        "            else:\n",
        "                counter += 1\n",
        "\n",
        "            if counter > 100:\n",
        "                #print(\"Early stopping at Epoch: \", epoch)\n",
        "                break\n",
        "        return losses,validations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp3MfHAL_Gxb"
      },
      "source": [
        "## Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ANdNfer_HXO"
      },
      "outputs": [],
      "source": [
        "def set_train_val_test_split_frac(seed: int, data: Data, val_frac: float, test_frac: float):\n",
        "    num_nodes = data.y.shape[0]\n",
        "\n",
        "    val_size = ceil(val_frac * num_nodes)\n",
        "    test_size = ceil(test_frac * num_nodes)\n",
        "    train_size = num_nodes - val_size - test_size\n",
        "\n",
        "    nodes = list(range(num_nodes))\n",
        "\n",
        "    # Take same test set every time using development seed for robustness\n",
        "    random.seed(development_seed)\n",
        "    random.shuffle(nodes)\n",
        "    test_idx = sorted(nodes[:test_size])\n",
        "    nodes = [x for x in nodes if x not in test_idx]\n",
        "\n",
        "    # Take train / val split according to seed\n",
        "    random.seed(seed)\n",
        "    random.shuffle(nodes)\n",
        "    train_idx = sorted(nodes[:train_size])\n",
        "    val_idx = sorted(nodes[train_size:])\n",
        "\n",
        "    assert len(train_idx) + len(val_idx) + len(test_idx) == num_nodes\n",
        "\n",
        "    def get_mask(idx):\n",
        "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        mask[idx] = 1\n",
        "        return mask\n",
        "\n",
        "    data.train_mask = get_mask(train_idx)\n",
        "    data.val_mask = get_mask(val_idx)\n",
        "    data.test_mask = get_mask(test_idx)\n",
        "\n",
        "    return data\n",
        "\n",
        "def set_train_val_test_split(\n",
        "        seed: int,\n",
        "        data: Data,\n",
        "        development_frac: float = 0.5,\n",
        "        num_per_class: int = 20) -> Data:\n",
        "    rnd_state = np.random.RandomState(development_seed)\n",
        "    num_nodes = data.y.shape[0]\n",
        "\n",
        "    #num_development = ceil(development_frac * num_nodes)\n",
        "    num_development = 1500 #ceil(development_frac * num_nodes)\n",
        "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
        "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
        "\n",
        "    train_idx = []\n",
        "    rnd_state = np.random.RandomState(seed)\n",
        "    for c in range(data.y.max() + 1):\n",
        "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
        "        train_idx.extend(rnd_state.choice(class_idx, min(num_per_class, ceil(len(class_idx) * 0.5)), replace=False))\n",
        "\n",
        "    val_idx = [i for i in development_idx if i not in train_idx]\n",
        "\n",
        "    def get_mask(idx):\n",
        "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        mask[idx] = 1\n",
        "        return mask\n",
        "\n",
        "    data.train_mask = get_mask(train_idx)\n",
        "    data.val_mask = get_mask(val_idx)\n",
        "    data.test_mask = get_mask(test_idx)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvQ0Xw1S-i9y"
      },
      "source": [
        "## Curvature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-WpHj9g-i9y"
      },
      "outputs": [],
      "source": [
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], int32[:,:], int32[:,:], float32[:], float32[:], int32, float32[:,:],boolean)\"\n",
        ")\n",
        "\n",
        "def _balanced_forman_curvature_undirected_personal(A, A2,edge_index,indices_neigh,d_in, d_out, N, C,fcc = True):\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    if (i < N) and (j < N):\n",
        "        if A[i, j] == 0:\n",
        "            C[i, j] = 0\n",
        "            return\n",
        "\n",
        "        if d_in[i] > d_out[j]:\n",
        "            d_max = d_in[i]\n",
        "            d_min = d_out[j]\n",
        "        else:\n",
        "            d_max = d_out[j]\n",
        "            d_min = d_in[i]\n",
        "\n",
        "        if d_min == 1:\n",
        "           C[i, j] = 0\n",
        "           return\n",
        "\n",
        "        C[i, j] = ((2 / d_max) + (2 / d_min) - 2\n",
        "                    + (2 / d_max + 1 / d_min) * A2[i, j] * A[i, j]\n",
        "                  )\n",
        "        if fcc:\n",
        "            ind1_i,ind2_i = indices_neigh[i,0], indices_neigh[i,1]\n",
        "            neighs_i = edge_index[1,ind1_i:ind2_i]\n",
        "\n",
        "            ind1_j,ind2_j = indices_neigh[j,0],indices_neigh[j,1]\n",
        "            neighs_j = edge_index[1,ind1_j:ind2_j]\n",
        "\n",
        "\n",
        "            sharp_ij = 0\n",
        "            lambda_ij = 0\n",
        "            for k_count in range(len(neighs_i)):\n",
        "                k = neighs_i[k_count]\n",
        "\n",
        "                ind1_k = indices_neigh[k,0]\n",
        "                ind2_k = indices_neigh[k,1]\n",
        "                neighs_k = edge_index[1,ind1_k:ind2_k]\n",
        "                if A[k,i]*(1-A[k,j]) !=0 and k != j: #Only have k in S(i)\\S(j)\n",
        "\n",
        "                    had = 0\n",
        "                    for l_count in range(len(neighs_k)):\n",
        "                        l = neighs_k[l_count]\n",
        "                        had += A[k,l]*A[i,l]*A[j,l]\n",
        "\n",
        "                    TMP =A[k,i]*(1-A[k,j])*(A2[k,j] -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_ij += 1\n",
        "                        if TMP > lambda_ij:\n",
        "                            lambda_ij = TMP\n",
        "\n",
        "            for k_count in range(len(neighs_j)):\n",
        "                k = neighs_j[k_count]\n",
        "\n",
        "                ind1_k,ind2_k = indices_neigh[k,0],indices_neigh[k,1]\n",
        "                neighs_k = edge_index[1,ind1_k:ind2_k]\n",
        "\n",
        "                if A[j,k]*(1-A[k,i]) !=0 and k != i: #Only have k in S(j)\\S(i)\n",
        "                    had = 0\n",
        "\n",
        "                    for l_count in range(len(neighs_k)):\n",
        "                        l = neighs_k[l_count]\n",
        "                        had += A[k,l]*A[i,l]*A[j,l]\n",
        "\n",
        "                    TMP = A[j,k]*(1-A[k,i])*(A2[k,i] -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_ij += 1\n",
        "                        if TMP > lambda_ij:\n",
        "                            lambda_ij = TMP\n",
        "\n",
        "            if lambda_ij > 0:\n",
        "                C[i, j] += sharp_ij / (d_max * lambda_ij)\n",
        "\n",
        "\n",
        "def balanced_forman_curvature_undirected_personal(A,edge_index, C=None,fcc = True):\n",
        "    N = A.shape[0]\n",
        "    threadsperblock = (32,16)#,10)\n",
        "    blockspergrid_x = math.ceil(N / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(N / threadsperblock[1])\n",
        "\n",
        "    blockspergrid_2d = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "    A2 = torch.matmul(A, A)\n",
        "\n",
        "    d_in = A.sum(axis=0)\n",
        "    d_out = A.sum(axis=1)\n",
        "\n",
        "    ind1 = 0\n",
        "    ind2 = 0\n",
        "    index_tuples = []\n",
        "    for k in range(N):#test:\n",
        "        ind2 += int(d_in[k].item())\n",
        "        index_tuples.append((ind1,ind2))\n",
        "        ind1 = ind2\n",
        "    index_tuples = torch.tensor(index_tuples).cuda()\n",
        "\n",
        "    if C is None:\n",
        "        C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    _balanced_forman_curvature_undirected_personal[blockspergrid_2d, threadsperblock](A, A2,edge_index,index_tuples,d_in, d_out, N, C,fcc)\n",
        "    return C\n",
        "\n",
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], float32[:], float32[:], int32, float32[:,:])\"\n",
        ")\n",
        "def _balanced_forman_curvature_jctopping(A, A2, d_in, d_out, N, C):\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    if (i < N) and (j < N):\n",
        "        if A[i, j] == 0:\n",
        "            C[i, j] = 0\n",
        "            return\n",
        "\n",
        "        if d_in[i] > d_out[j]:\n",
        "            d_max = d_in[i]\n",
        "            d_min = d_out[j]\n",
        "        else:\n",
        "            d_max = d_out[j]\n",
        "            d_min = d_in[i]\n",
        "\n",
        "        if d_max * d_min == 0:\n",
        "            C[i, j] = 0\n",
        "            return\n",
        "\n",
        "        sharp_ij = 0\n",
        "        lambda_ij = 0\n",
        "        for k in range(N):\n",
        "            TMP = A[k, j] * (A2[i, k] - A[i, k]) * A[i, j]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "            TMP = A[i, k] * (A2[k, j] - A[k, j]) * A[i, j]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "        C[i, j] = (\n",
        "            (2 / d_max) + (2 / d_min) - 2 + (2 / d_max + 1 / d_min) * A2[i, j] * A[i, j]\n",
        "        )\n",
        "        if lambda_ij > 0:\n",
        "            C[i, j] += sharp_ij / (d_max * lambda_ij)\n",
        "\n",
        "\n",
        "def balanced_forman_curvature_jctopping(A, C=None):\n",
        "    N = A.shape[0]\n",
        "    A2 = torch.matmul(A, A)\n",
        "    d_in = A.sum(axis=0)\n",
        "    d_out = A.sum(axis=1)\n",
        "    if C is None:\n",
        "        C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    threadsperblock = (16, 16)\n",
        "    blockspergrid_x = math.ceil(N / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(N / threadsperblock[1])\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "    _balanced_forman_curvature_jctopping[blockspergrid, threadsperblock](A, A2, d_in, d_out, N, C)\n",
        "    return C\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcrL8toT-i9z"
      },
      "source": [
        "## SDRF Cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDmdEno9-i9z"
      },
      "outputs": [],
      "source": [
        "NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS=0\n",
        "\n",
        "def softmax(a, tau=1):\n",
        "    exp_a = np.exp(a * tau)\n",
        "    return exp_a / exp_a.sum()\n",
        "\n",
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], int32[:,:], int32[:,:], float32, float32, int32, float32[:,:], int32, int32, int32[:], int32[:], int32, int32,boolean)\"\n",
        ")\n",
        "\n",
        "def _curvature_post_rewiring_undirected_personal( A, A2,edge_index,indices_neigh, d_in_x, d_out_y, N, D, x, y, i_neighbors, j_neighbors, dim_i, dim_j,fcc = True\n",
        "):\n",
        "    I, J = cuda.grid(2)\n",
        "\n",
        "    if (I < dim_i) and (J < dim_j):\n",
        "        i = i_neighbors[I]\n",
        "        j = j_neighbors[J]\n",
        "\n",
        "        if (i == j) or (A[i, j] != 0):\n",
        "            D[I, J] = -1000\n",
        "            return\n",
        "\n",
        "        A_i_j = A[i, j]\n",
        "        A_i_j += 1\n",
        "\n",
        "        if i == x:\n",
        "            d_in_x += 1\n",
        "        elif j == y:\n",
        "            d_out_y += 1\n",
        "\n",
        "\n",
        "        if d_in_x > d_out_y:\n",
        "            d_max = d_in_x\n",
        "            d_min = d_out_y\n",
        "        else:\n",
        "            d_max = d_out_y\n",
        "            d_min = d_in_x\n",
        "\n",
        "        if d_min ==1:#d_in_x * d_out_y == 0:\n",
        "            D[I, J] = 0\n",
        "            return\n",
        "\n",
        "        A2_x_y = A2[x, y]\n",
        "        # Difference in triangles term\n",
        "        if (x == i) and (A[j, y] != 0):\n",
        "            A2_x_y += 1.\n",
        "        elif (y == j) and (A[x, i] != 0):\n",
        "            A2_x_y += 1.\n",
        "\n",
        "        # Difference in four-cycles term\n",
        "        ind1_x,ind2_x = indices_neigh[x,0], indices_neigh[x,1]\n",
        "        neighs_x = edge_index[1,ind1_x:ind2_x]\n",
        "\n",
        "        ind1_y,ind2_y = indices_neigh[y,0],indices_neigh[y,1]\n",
        "        neighs_y = edge_index[1,ind1_y:ind2_y]\n",
        "\n",
        "        D[I, J] = (\n",
        "                (2 / d_max)\n",
        "                + (2 / d_min)\n",
        "                - 2\n",
        "                + (2 / d_max + 1 / d_min) * A2_x_y * A[x, y]\n",
        "            )\n",
        "\n",
        "        if fcc:\n",
        "\n",
        "            sharp_xy = 0\n",
        "            lambda_xy = 0\n",
        "\n",
        "\n",
        "            A_x_j = A[x,j] + 0\n",
        "            if i == x and y !=j:\n",
        "                A_x_j += 1\n",
        "\n",
        "            for k_count in range(len(neighs_x)):\n",
        "                k = neighs_x[k_count]\n",
        "\n",
        "                ind1_k = indices_neigh[k,0]\n",
        "                ind2_k = indices_neigh[k,1]\n",
        "                neighs_k = edge_index[1,ind1_k:ind2_k]\n",
        "\n",
        "                if k != i and k != j and y !=i and y!=j:\n",
        "                    A2_k_y = A2[k, y]\n",
        "                elif k ==i and y !=j:\n",
        "                    A2_k_y = A2[k, y] + A[j,y]\n",
        "                elif k ==j and y !=i:\n",
        "                    A2_k_y = A2[k, y] + A[i,y]\n",
        "                elif k!=j and y==i:\n",
        "                    A2_k_y = A2[k, y] + A[k,j]\n",
        "                elif k!=i and y==j:\n",
        "                    A2_k_y = A2[k, y] + A[k,i]\n",
        "                elif (k ==i and y ==j) or (k ==j and y == i):\n",
        "                    A2_k_y = A2[k, y] + +1*A[k,k] + 1*A[y,y]\n",
        "\n",
        "                A_k_y = A[k,y] + 0\n",
        "                A_x_k = A[x,k] + 0\n",
        "\n",
        "                if  (k == i and j ==y) or (k == y and j == i):\n",
        "                    A_k_y +=1\n",
        "                if (i == x and k == j) or (x==j and k == i):\n",
        "                    A_x_k +=1\n",
        "\n",
        "                if A_x_k*(1-A_k_y) !=0 and k!=y:\n",
        "\n",
        "                    had = 0\n",
        "                    for l_count in range(len(neighs_k)): #This doesn't sum over j since we haven't adapted the edge index yet\n",
        "                        l = neighs_k[l_count]\n",
        "                        A_k_l = A[k,l] + 0\n",
        "                        A_x_l = A[x,l] + 0\n",
        "                        A_y_l = A[y,l] + 0\n",
        "                        if (k == i and l == j) or (k == j and l == i):\n",
        "                            A_k_l +=1\n",
        "                        if (x == i and l == j) or (x == j and l == i):\n",
        "                            A_x_l +=1\n",
        "                        if (y == i and l == j) or (y == j and l == i):\n",
        "                            A_y_l +=1\n",
        "                        had += A_k_l*A_x_l*A_y_l\n",
        "\n",
        "                    TMP =A_x_k*(1-A_k_y)*(A2_k_y -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_xy += 1\n",
        "                        if TMP > lambda_xy:\n",
        "                            lambda_xy = TMP\n",
        "\n",
        "            for w_count in range(len(neighs_y)):\n",
        "                w = neighs_y[w_count]\n",
        "                ind1_w = indices_neigh[w,0]\n",
        "                ind2_w = indices_neigh[w,1]\n",
        "                neighs_w = edge_index[1,ind1_w:ind2_w]\n",
        "\n",
        "                if w != i and w != j and x !=i and x!=j:\n",
        "                    A2_w_x = A2[w, x]\n",
        "                elif w ==i and x !=j:\n",
        "                    A2_w_x = A2[w, x] + A[j,x]\n",
        "                elif w ==j and x !=i:\n",
        "                    A2_w_x = A2[w, x] + A[i,x]\n",
        "                elif w!=j and x==i:\n",
        "                    A2_w_x = A2[w, x] + A[w,j]\n",
        "                elif w!=i and x==j:\n",
        "                    A2_w_x = A2[w, x] + A[w,i]\n",
        "                elif (w ==i and x ==j) or (w ==j and x == i):\n",
        "                    A2_w_x = A2[w, x] +1*A[w,w] + 1*A[x,x]\n",
        "\n",
        "                A_x_w = A[x,w] + 0\n",
        "                if  w ==j and x ==i:\n",
        "                    A_x_w +=1\n",
        "\n",
        "                A_y_w = A[y,w] + 0\n",
        "                A_w_x = A[w,x] + 0\n",
        "\n",
        "                if  (w == i and j ==y) or (w == y and j == i):\n",
        "                        A_y_w +=1\n",
        "                if (i == x and w == j) or (x==j and w == i):\n",
        "                        A_w_x +=1\n",
        "\n",
        "                if A_y_w*(1-A_w_x) !=0 and w != x:\n",
        "                    had = 0\n",
        "                    for l_count in range(len(neighs_w)): # If w ==j (SHOULD NEVER HAPPEN), this doesn't sum over i since we haven't adapted the edge index yet\n",
        "                        l = neighs_w[l_count]\n",
        "                        A_w_l = A[w,l] + 0\n",
        "                        A_x_l = A[x,l] + 0\n",
        "                        A_y_l = A[y,l] + 0\n",
        "                        if (w == i and l == j) or (w == j and l == i):\n",
        "                            A_w_l +=1\n",
        "                        if (x == i and l == j) or (x == j and l == i):\n",
        "                            A_x_l +=1\n",
        "                        if (y == i and l == j) or (y == j and l == i):\n",
        "                            A_y_l +=1\n",
        "\n",
        "                        had += A_w_l*A_x_l*A_y_l\n",
        "\n",
        "\n",
        "                    TMP = A_y_w*(1-A_x_w)*(A2_w_x -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_xy +=  1\n",
        "                        if TMP > lambda_xy:\n",
        "                            lambda_xy = TMP\n",
        "\n",
        "\n",
        "            if lambda_xy > 0:\n",
        "                D[I, J] += sharp_xy / (d_max * lambda_xy)\n",
        "\n",
        "\n",
        "def curvature_post_rewiring_personal(A, x, y,edge_index, i_neighbors, j_neighbors, D=None,is_undirected = False,fcc = True):\n",
        "\n",
        "    N = A.shape[0]\n",
        "    A2 = torch.matmul(A, A)\n",
        "    d_in = A.sum(axis = 0)#A[:, x].sum()\n",
        "    d_out = A.sum(axis = 1)#A[y].sum()\n",
        "    if D is None:\n",
        "        D = torch.zeros(len(i_neighbors), len(j_neighbors)).cuda()\n",
        "\n",
        "    ind1 = 0\n",
        "    ind2 = 0\n",
        "    index_tuples = []\n",
        "    for k in range(N):\n",
        "        ind2 += int(d_in[k].item())\n",
        "        index_tuples.append((ind1,ind2))\n",
        "        ind1 = ind2\n",
        "    index_tuples = torch.tensor(index_tuples).cuda()\n",
        "\n",
        "    d_in = d_in[x]\n",
        "    d_out = d_out[y]\n",
        "    threadsperblock = (16, 16)\n",
        "    blockspergrid_x = math.ceil(D.shape[0] / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(D.shape[1] / threadsperblock[1])\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "    if is_undirected:\n",
        "        _curvature_post_rewiring_undirected_personal[blockspergrid, threadsperblock](\n",
        "            A,\n",
        "            A2,\n",
        "            edge_index,\n",
        "            index_tuples,\n",
        "            d_in,\n",
        "            d_out,\n",
        "            N,\n",
        "            D,\n",
        "            x,\n",
        "            y,\n",
        "            np.array(i_neighbors),\n",
        "            np.array(j_neighbors),\n",
        "            D.shape[0],\n",
        "            D.shape[1],\n",
        "            fcc\n",
        "        )\n",
        "    else:\n",
        "        print(\"Not implemented for directed graphs\")\n",
        "        return\n",
        "    return D\n",
        "\n",
        "\n",
        "def sdrf_cuda_personal(\n",
        "    data,\n",
        "    loops=10,\n",
        "    remove_edges=False,\n",
        "    removal_bound=0.5,\n",
        "    tau=1,\n",
        "    int_node = False,\n",
        "    is_undirected=False,\n",
        "    fcc = True\n",
        "):\n",
        "    N = data.num_nodes\n",
        "    G_in = torch_geometric.utils.to_networkx(data)\n",
        "\n",
        "    if is_undirected:\n",
        "        G_in = G_in.to_undirected()\n",
        "\n",
        "    #print(\"Start\", G_in)\n",
        "    count_edge_removal = 0\n",
        "\n",
        "    A = torch.tensor(nx.adjacency_matrix(G_in).todense(), dtype = torch.float)\n",
        "    A = A.cuda()\n",
        "\n",
        "\n",
        "    edge_index = data.edge_index.clone()\n",
        "    edge_index = edge_index.cuda()\n",
        "    N = A.shape[0]\n",
        "\n",
        "    C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    for idx in range(loops):\n",
        "\n",
        "        count_new_node = len(G_in.nodes)\n",
        "        can_add = True\n",
        "        if is_undirected:\n",
        "            balanced_forman_curvature_undirected_personal(A,edge_index ,C=C,fcc = fcc)\n",
        "        else:\n",
        "            print(\"Not implemented for directed graphs\")\n",
        "            return\n",
        "\n",
        "\n",
        "        ix_min = C.argmin()\n",
        "\n",
        "\n",
        "        x =  torch.div(ix_min,N,rounding_mode='trunc')\n",
        "        y = ix_min % N\n",
        "\n",
        "        x = x.item()\n",
        "        y = y.item()\n",
        "\n",
        "\n",
        "        if is_undirected:\n",
        "            x_neighbors = list(G_in.neighbors(x)) + [x] # !! We're adding x to the set of neighbours\n",
        "            y_neighbors = list(G_in.neighbors(y)) + [y]\n",
        "        else:\n",
        "            x_neighbors = list(G_in.successors(x)) + [x]\n",
        "            y_neighbors = list(G_in.predecessors(y)) + [y]\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "\n",
        "        for i in x_neighbors:\n",
        "            for j in y_neighbors:\n",
        "                if (i != j) and (not G_in.has_edge(i, j)):\n",
        "                    candidates.append((i, j))\n",
        "\n",
        "        if len(candidates):\n",
        "            D = curvature_post_rewiring_personal(A,x,y,edge_index,x_neighbors,y_neighbors,D=None,is_undirected=is_undirected,fcc = fcc)\n",
        "            improvements = []\n",
        "            for (i, j) in candidates:\n",
        "                improvements.append(\n",
        "                    (D-C[x,y])[x_neighbors.index(i), y_neighbors.index(j)].item()\n",
        "                )\n",
        "            k, l = candidates[np.random.choice(range(len(candidates)), p=softmax(np.array(improvements), tau=tau))] ##For directed graph: Makes sense: k is selected uit of \"i\" and \"l\" out of j\n",
        "\n",
        "            if int_node:\n",
        "                A = F.pad(input=A, pad=(0,1,0,1), mode='constant', value=0)\n",
        "                G_in.add_node(count_new_node)\n",
        "                G_in.add_edge(k,count_new_node)\n",
        "                G_in.add_edge(count_new_node, l)\n",
        "                if is_undirected:\n",
        "                    A[k, count_new_node] = A[count_new_node, l] = 1.\n",
        "                    A[count_new_node, k] = A[l, count_new_node] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                else:\n",
        "                    A[k, count_new_node] = A[count_new_node, l] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "            else:\n",
        "                G_in.add_edge(k, l)\n",
        "                if is_undirected:\n",
        "                    A[k, l] = A[l, k] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                else:\n",
        "                    A[k, l] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "\n",
        "        else:\n",
        "            can_add = False\n",
        "            if not remove_edges:\n",
        "                break\n",
        "\n",
        "        if remove_edges:\n",
        "            ix_max = C.argmax()\n",
        "            xmax = torch.div(ix_max,N,rounding_mode='trunc').item()\n",
        "            ymax = (ix_max % N).item()\n",
        "            if C[xmax, ymax] > removal_bound:\n",
        "                G_in.remove_edge(xmax, ymax)\n",
        "\n",
        "                if is_undirected:\n",
        "                    A[xmax, ymax] = A[ymax, xmax] = 0.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                else:\n",
        "                    A[xmax, ymax] = 0.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                count_edge_removal += 1\n",
        "\n",
        "            else:\n",
        "                if can_add is False:\n",
        "                    break\n",
        "\n",
        "        #Dcomputed = D[x_neighbors.index(k), y_neighbors.index(l)].item()\n",
        "        #Cnew = balanced_forman_curvature_undirected_personal(A,edge_index ,fcc = fcc)\n",
        "        #print(Cnew[x,y])\n",
        "        #print(Dcomputed)\n",
        "    return G_in,count_edge_removal\n",
        "\n",
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], float32, float32, int32, float32[:,:], int32, int32, int32[:], int32[:], int32, int32)\"\n",
        ")\n",
        "def _balanced_forman_post_delta_jctopping(\n",
        "    A, A2, d_in_x, d_out_y, N, D, x, y, i_neighbors, j_neighbors, dim_i, dim_j\n",
        "):\n",
        "    I, J = cuda.grid(2)\n",
        "\n",
        "    if (I < dim_i) and (J < dim_j):\n",
        "        i = i_neighbors[I]\n",
        "        j = j_neighbors[J]\n",
        "\n",
        "        if (i == j) or (A[i, j] != 0):\n",
        "            D[I, J] = -1000\n",
        "            return\n",
        "\n",
        "        # Difference in degree terms\n",
        "        if j == x:\n",
        "            d_in_x += 1\n",
        "        elif i == y:\n",
        "            d_out_y += 1\n",
        "\n",
        "        if d_in_x * d_out_y == 0:\n",
        "            D[I, J] = 0\n",
        "            return\n",
        "\n",
        "        if d_in_x > d_out_y:\n",
        "            d_max = d_in_x\n",
        "            d_min = d_out_y\n",
        "        else:\n",
        "            d_max = d_out_y\n",
        "            d_min = d_in_x\n",
        "\n",
        "        # Difference in triangles term\n",
        "        A2_x_y = A2[x, y]\n",
        "        if (x == i) and (A[j, y] != 0):\n",
        "            A2_x_y += A[j, y]\n",
        "        elif (y == j) and (A[x, i] != 0):\n",
        "            A2_x_y += A[x, i]\n",
        "\n",
        "        # Difference in four-cycles term\n",
        "        sharp_ij = 0\n",
        "        lambda_ij = 0\n",
        "        for z in range(N):\n",
        "            A_z_y = A[z, y] + 0\n",
        "            A_x_z = A[x, z] + 0\n",
        "            A2_z_y = A2[z, y] + 0\n",
        "            A2_x_z = A2[x, z] + 0\n",
        "\n",
        "            if (z == i) and (y == j):\n",
        "                A_z_y += 1\n",
        "            if (x == i) and (z == j):\n",
        "                A_x_z += 1\n",
        "            if (z == i) and (A[j, y] != 0):\n",
        "                A2_z_y += A[j, y]\n",
        "            if (x == i) and (A[j, z] != 0):\n",
        "                A2_x_z += A[j, z]\n",
        "            if (y == j) and (A[z, i] != 0):\n",
        "                A2_z_y += A[z, i]\n",
        "            if (z == j) and (A[x, i] != 0):\n",
        "                A2_x_z += A[x, i]\n",
        "\n",
        "            TMP = A_z_y * (A2_x_z - A_x_z) * A[x, y]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "            TMP = A_x_z * (A2_z_y - A_z_y) * A[x, y]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "        D[I, J] = (\n",
        "            (2 / d_max) + (2 / d_min) - 2 + (2 / d_max + 1 / d_min) * A2_x_y * A[x, y]\n",
        "        )\n",
        "        if lambda_ij > 0:\n",
        "            D[I, J] += sharp_ij / (d_max * lambda_ij)\n",
        "\n",
        "\n",
        "def balanced_forman_post_delta_jctopping(A, x, y, i_neighbors, j_neighbors, D=None):\n",
        "    N = A.shape[0]\n",
        "    A2 = torch.matmul(A, A)\n",
        "    d_in = A[:, x].sum()\n",
        "    d_out = A[y].sum()\n",
        "    if D is None:\n",
        "        D = torch.zeros(len(i_neighbors), len(j_neighbors)).cuda()\n",
        "\n",
        "    threadsperblock = (16, 16)\n",
        "    blockspergrid_x = math.ceil(D.shape[0] / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(D.shape[1] / threadsperblock[1])\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "    _balanced_forman_post_delta_jctopping[blockspergrid, threadsperblock](\n",
        "        A,\n",
        "        A2,\n",
        "        d_in,\n",
        "        d_out,\n",
        "        N,\n",
        "        D,\n",
        "        x,\n",
        "        y,\n",
        "        np.array(i_neighbors),\n",
        "        np.array(j_neighbors),\n",
        "        D.shape[0],\n",
        "        D.shape[1],\n",
        "    )\n",
        "    return D\n",
        "\n",
        "\n",
        "def sdrf_jctopping(\n",
        "    data,\n",
        "    loops=10,\n",
        "    remove_edges=True,\n",
        "    removal_bound=0.5,\n",
        "    tau=1,\n",
        "    is_undirected=False,\n",
        "):\n",
        "    edge_index = data.edge_index\n",
        "    if is_undirected:\n",
        "        edge_index = torch_geometric.utils.to_undirected(edge_index)\n",
        "    A = torch_geometric.utils.to_dense_adj(torch_geometric.utils.remove_self_loops(edge_index)[0])[0]\n",
        "    N = A.shape[0]\n",
        "    G = torch_geometric.utils.to_networkx(data)\n",
        "    if is_undirected:\n",
        "        G = G.to_undirected()\n",
        "    A = A.cuda()\n",
        "    C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    for x in tqdm(range(loops)):\n",
        "        can_add = True\n",
        "        balanced_forman_curvature_jctopping(A, C=C)\n",
        "        ix_min = C.argmin().item()\n",
        "        x = ix_min // N\n",
        "        y = ix_min % N\n",
        "\n",
        "        if is_undirected:\n",
        "            x_neighbors = list(G.neighbors(x)) + [x]\n",
        "            y_neighbors = list(G.neighbors(y)) + [y]\n",
        "        else:\n",
        "            x_neighbors = list(G.successors(x)) + [x]\n",
        "            y_neighbors = list(G.predecessors(y)) + [y]\n",
        "        candidates = []\n",
        "        for i in x_neighbors:\n",
        "            for j in y_neighbors:\n",
        "                if (i != j) and (not G.has_edge(i, j)):\n",
        "                    candidates.append((i, j))\n",
        "\n",
        "        if len(candidates):\n",
        "            D = balanced_forman_post_delta_jctopping(A, x, y, x_neighbors, y_neighbors)\n",
        "            improvements = []\n",
        "            for (i, j) in candidates:\n",
        "                improvements.append(\n",
        "                    (D - C[x, y])[x_neighbors.index(i), y_neighbors.index(j)].item()\n",
        "                )\n",
        "\n",
        "            k, l = candidates[\n",
        "                np.random.choice(\n",
        "                    range(len(candidates)), p=softmax(np.array(improvements), tau=tau)\n",
        "                )\n",
        "            ]\n",
        "            G.add_edge(k, l)\n",
        "            if is_undirected:\n",
        "                A[k, l] = A[l, k] = 1\n",
        "            else:\n",
        "                A[k, l] = 1\n",
        "        else:\n",
        "            can_add = False\n",
        "            if not remove_edges:\n",
        "                break\n",
        "\n",
        "        if remove_edges:\n",
        "            ix_max = C.argmax().item()\n",
        "            x = ix_max // N\n",
        "            y = ix_max % N\n",
        "            if C[x, y] > removal_bound:\n",
        "                G.remove_edge(x, y)\n",
        "                if is_undirected:\n",
        "                    A[x, y] = A[y, x] = 0\n",
        "                else:\n",
        "                    A[x, y] = 0\n",
        "            else:\n",
        "                if can_add is False:\n",
        "                    break\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RBd3V8U-i90"
      },
      "source": [
        "# Experiments Rewiring Sweeps: Undirected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g53Q-agW-i91",
        "outputId": "a4b1d080-9f5d-4b1a-d426-755c8b5ea26a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Parameters for the experiment\n",
        "\"\"\"\n",
        "\n",
        "datasetname = \"Texas\"\n",
        "results_dir = \"results\"\n",
        "rewiring_run = True\n",
        "make_undirected = True\n",
        "int_node = False\n",
        "Curvature_type = \"BFC_no4cycle\"\n",
        "\n",
        "path = \"\"\n",
        "\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "os.environ[\"NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS\"] = \"false\"\n",
        "\n",
        "\n",
        "dataset,data,G = load_data(datasetname)\n",
        "dataset_lcc = lcc_dataset(dataset,to_undirected = make_undirected)\n",
        "data_lcc = dataset_lcc[0]\n",
        "\n",
        "data_information(dataset_lcc,data_lcc)\n",
        "\n",
        "\n",
        "sweep_configuration = hyperparameters_Neurips_1[datasetname]\n",
        "\n",
        "NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS=0\n",
        "\n",
        "\n",
        "sweep_configuration[\"name\"] = datasetname + '_' + Curvature_type\n",
        "\n",
        "\n",
        "def objective(config,rewire = False):\n",
        "    accuracies = []\n",
        "    test_acc = []\n",
        "    if rewire:\n",
        "        print(\"===Starting Rewiring===\")\n",
        "        G_rewired,edge_index_rewired = create_rewired_edge_index(data_lcc,config,intermediate_node=int_node,remove_edges=True,curvaturetype=Curvature_type)\n",
        "        print(\" \")\n",
        "    print(\" == Starting Runs == \")\n",
        "    for idx_k,k in tqdm(enumerate(val_seeds)):\n",
        "        if datasetname == \"Cora\" or datasetname == \"Citeseer\" or datasetname == \"Pubmed\":\n",
        "            data_undirected_split = set_train_val_test_split(k,data_lcc)\n",
        "        else:\n",
        "            data_undirected_split = set_train_val_test_split_frac(k,data_lcc,0.2,0.2)\n",
        "\n",
        "        if rewire:\n",
        "            delta = len(G_rewired.nodes) - data_undirected_split.num_nodes\n",
        "            if delta != 0:\n",
        "                print(\"Additional Nodes added: \", delta)\n",
        "\n",
        "                # pad(left, right, top, bottom)\n",
        "                new_x = F.pad(input=data.x, pad=(0, 0, 0,delta), mode='constant', value=0)\n",
        "                new_y = F.pad(input=data.y, pad=(0,delta), mode='constant', value=0)\n",
        "                new_train_mask = F.pad(input=data.train_mask, pad=(0,delta), mode='constant', value=False)\n",
        "                new_val_mask = F.pad(input=data.val_mask, pad=(0,delta), mode='constant', value=False)\n",
        "                new_test_mask = F.pad(input=data.test_mask, pad=(0,delta), mode='constant', value=False)\n",
        "\n",
        "                data_undirected_split = Data(x = new_x, edge_index = edge_index_rewired, y = new_y,train_mask = new_train_mask,val_mask = new_val_mask,test_mask = new_test_mask,is_undirected = True)\n",
        "            else:\n",
        "                data_undirected_split.edge_index = edge_index_rewired\n",
        "\n",
        "        data_undirected_split.to(device)\n",
        "\n",
        "        Exp = Experiment(device,datasetname,dataset_lcc,data_undirected_split,config)\n",
        "\n",
        "\n",
        "        counter = 0\n",
        "        for epoch in range(1, Exp.epoch):\n",
        "            loss = Exp.train()\n",
        "            val = Exp.validate()\n",
        "            wandb.log({\"loss \" + str(idx_k): loss, \"val \" + str(idx_k): val,\"epoch\": epoch})\n",
        "            if epoch ==1:\n",
        "                best_val = val\n",
        "            elif epoch > 1 and val > best_val:\n",
        "                best_val = val\n",
        "                counter = 0\n",
        "            else:\n",
        "                counter += 1\n",
        "            if counter > 100:\n",
        "                break\n",
        "        final_accuracy = Exp.validate()\n",
        "        final_test_acc = Exp.test()\n",
        "        accuracies.append(final_accuracy)\n",
        "        test_acc.append(final_test_acc)\n",
        "    print(\"\")\n",
        "    return np.mean(np.array(accuracies)),np.mean(np.array(test_acc))\n",
        "\n",
        "def create_rewired_edge_index(data,hyperparameters,intermediate_node,remove_edges,curvaturetype: str ):\n",
        "    if curvaturetype == \"BFC_w4cycle\":\n",
        "        G_rewired,_ = sdrf_cuda_personal(\n",
        "        data,\n",
        "        loops=hyperparameters[\"loops\"],\n",
        "        remove_edges=remove_edges,\n",
        "        removal_bound=hyperparameters[\"C+\"],\n",
        "        tau=hyperparameters[\"tau\"],\n",
        "        int_node = intermediate_node,\n",
        "        is_undirected=data.is_undirected(),\n",
        "        fcc = True\n",
        "                        )\n",
        "    elif curvaturetype == \"BFC_no4cycle\":\n",
        "        G_rewired,_ = sdrf_cuda_personal(\n",
        "        data,\n",
        "        loops=hyperparameters[\"loops\"],\n",
        "        remove_edges=remove_edges,\n",
        "        removal_bound=hyperparameters[\"C+\"],\n",
        "        tau=hyperparameters[\"tau\"],\n",
        "        int_node = intermediate_node,\n",
        "        is_undirected=data.is_undirected(),\n",
        "        fcc = False\n",
        "                        )\n",
        "    elif curvaturetype == \"JcT\":\n",
        "        G_rewired = sdrf_jctopping(\n",
        "        data,\n",
        "        loops=hyperparameters[\"loops\"],\n",
        "        remove_edges=remove_edges,\n",
        "        removal_bound=hyperparameters[\"C+\"],\n",
        "        tau=hyperparameters[\"tau\"],\n",
        "        is_undirected=data.is_undirected(),\n",
        "                        )\n",
        "\n",
        "    edge_index_rewired = torch_geometric.utils.to_undirected(torch.tensor(list(G_rewired.edges)).t())\n",
        "    return G_rewired,edge_index_rewired\n",
        "\n",
        "def main():\n",
        "    wandb.init(dir = \"\")\n",
        "    acc,test_acc = objective(wandb.config,rewiring_run)\n",
        "    #wandb.run.summary[\"mean accuracy\"] = acc\n",
        "    wandb.log({\"mean accuracy\": acc, \"mean test accuracy\": test_acc})\n",
        "\n",
        "sweep_id = \"9f46xixw\" # wandb.sweep(sweep=sweep_configuration, project=\"curvature\")\n",
        "wandb.agent(sweep_id, project=\"Curvature_Neurips\", function=main,count = 150)\n",
        "\n",
        "#sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"Curvature_Neurips\")\n",
        "#wandb.agent(sweep_id, function=main,count = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
