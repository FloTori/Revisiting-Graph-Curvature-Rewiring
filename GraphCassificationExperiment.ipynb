{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCkJMjtW-i9o",
        "outputId": "89859d2b-9eb8-431d-9677-3af93c319091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device:  cuda\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "\n",
        "from scipy import sparse\n",
        "import math\n",
        "from numba import cuda\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"using device: \", device)\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n",
        "import json\n",
        "\n",
        "import random as random\n",
        "from math import ceil\n",
        "\n",
        "import wandb\n",
        "\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGQHD3Hv_fuE",
        "outputId": "60eacca0-a214-4669-bc33-19b080883f90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflotori\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Floriano Tori\\.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key='aa9268b8188bf556e7e7204bf67cf461622fc09b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nRoKGCof-i9q"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auSq9JTM-i9s"
      },
      "source": [
        "## Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "adZSSFZS-i9t"
      },
      "outputs": [],
      "source": [
        "val_seeds = [4258031807,\n",
        "    3829679737,\n",
        "    3706579387,\n",
        "    789594926,\n",
        "    3628091752,\n",
        "    54121625,\n",
        "    825346923,\n",
        "    646393804,\n",
        "    1579300575,\n",
        "    246132812,\n",
        "    2882726575,\n",
        "    970387138,\n",
        "    413984459,\n",
        "    288449314,\n",
        "    1594895720,\n",
        "    1950255998,\n",
        "    4015021126,\n",
        "    3798842978,\n",
        "    2668546961,\n",
        "    1254814623,\n",
        "    1804908540,\n",
        "    674684671,\n",
        "    1988664841,\n",
        "    3361110162,\n",
        "    3784152546,\n",
        "    3431665473,\n",
        "    1487802115,\n",
        "    1080377472,\n",
        "    1033325667,\n",
        "    2068347440,\n",
        "    50862517,\n",
        "    1266130159,\n",
        "    3705237643,\n",
        "    2523113545,\n",
        "    1385697073,\n",
        "    1227694832,\n",
        "    198559329,\n",
        "    1464601500,\n",
        "    490478722,\n",
        "    3144635527,\n",
        "    4085231799,\n",
        "    2935399337,\n",
        "    3291449301,\n",
        "    2933074791,\n",
        "    1604475278,\n",
        "    2748278770,\n",
        "    1041151773,\n",
        "    2302537583,\n",
        "    1592364233,\n",
        "    1347718791,\n",
        "    2260302349,\n",
        "    2870906085,\n",
        "    3324642025,\n",
        "    3383731094,\n",
        "    3268345887,\n",
        "    3861549985,\n",
        "    1839485103,\n",
        "    2440976226,\n",
        "    1348632978,\n",
        "    1730263803,\n",
        "    3273174762,\n",
        "    2443236195,\n",
        "    2018253000,\n",
        "    3131053563,\n",
        "    2750855724,\n",
        "    2142840570,\n",
        "    133334446,\n",
        "    2906772286,\n",
        "    1676623629,\n",
        "    2799515439,\n",
        "    1950780225,\n",
        "    245027879,\n",
        "    974231345,\n",
        "    1019551316,\n",
        "    418468904,\n",
        "    3645979760,\n",
        "    2676444879,\n",
        "    2600212003,\n",
        "    243207504,\n",
        "    4050914577,\n",
        "    395869280,\n",
        "    3037389484,\n",
        "    319467089,\n",
        "    2091061953,\n",
        "    1121224029,\n",
        "    1506683900,\n",
        "    4265586951,\n",
        "    910928236,\n",
        "    1175970114,\n",
        "    2105285287,\n",
        "    3164711608,\n",
        "    3255599240,\n",
        "    894959334,\n",
        "    493067366,\n",
        "    3349051410,\n",
        "    511641138,\n",
        "    2487307261,\n",
        "    951126382,\n",
        "    530590201,\n",
        "    17966177,\n",
        "]\n",
        "\n",
        "development_seed = 1684992425"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZvEuhjB-i9t"
      },
      "source": [
        "## Dictionaries parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xIL2r04s-i9u"
      },
      "outputs": [],
      "source": [
        "hyperparameters_Neurips_2 = {\n",
        "    \"MUTAG\": {\n",
        "        \"method\": \"random\",\n",
        "        \"metric\": {\"goal\": \"maximize\", \"name\": \"mean accuracy\"},\n",
        "        \"parameters\": {\n",
        "            \"learning_rate\": {\"max\": 0.5555, \"min\": 0.0001},\n",
        "            \"layers\": {\"values\": [[16],[16,16],[16,16,16],[32],[32,32],[32,32,32],[64],[64,64],[64,64,64],[128],[128,128],[128,128,128]]},\n",
        "            \"dropout\":{\"max\": 0.9999,\"min\":0.0001},\n",
        "            \"weight_decay\":{\"max\": 0.5555, \"min\": 0.0001},\n",
        "\t\t\t\"loops\": {\"min\":80,\"max\":120},\n",
        "\t\t\t\"tau\":{\"min\": 25, \"max\": 216},\n",
        "\t\t\t\"C+\": {\"min\": 0.2, \"max\": 17.5}\n",
        "\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHhTrhKX-i9u"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxqyiq14-i9v"
      },
      "source": [
        "## Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GwBXkNv2-i9v"
      },
      "outputs": [],
      "source": [
        "DEFAULT_DATA_PATH = \"data\"\n",
        "\n",
        "def get_dataset(\n",
        "    name: str, data_dir=DEFAULT_DATA_PATH\n",
        "):\n",
        "    #path = os.path.join(data_dir, name)\n",
        "    path = DEFAULT_DATA_PATH\n",
        "    if name in [\"Cora\", \"Citeseer\", \"Pubmed\"]:\n",
        "        dataset = Planetoid(path, name)\n",
        "    elif name in [\"Computers\", \"Photo\"]:\n",
        "        dataset = Amazon(path, name)\n",
        "    elif name == \"CoauthorCS\":\n",
        "        dataset = Coauthor(path, \"CS\")\n",
        "    elif name in [\"Cornell\", \"Texas\", \"Wisconsin\"]:\n",
        "        dataset = WebKB(path, name)\n",
        "    elif name in [\"Chameleon\", \"Squirrel\"]:\n",
        "        dataset = WikipediaNetwork(path, name, geom_gcn_preprocess=True)\n",
        "    elif name == \"Actor\":\n",
        "        dataset = Actor(path, \"Actor\")\n",
        "    else:\n",
        "        raise Exception(f\"Unknown dataset: {name}\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data(name: str, make_undirected: bool = False):\n",
        "    dataset = get_dataset(name)\n",
        "    data = dataset[0]\n",
        "    G = torch_geometric.utils.to_networkx(data)\n",
        "    if data.is_undirected() or make_undirected:#undirected:\n",
        "        G = G.to_undirected() #This is for Networkx to represent it as a undirected Graph (Otherwise it would 'plot' i->j and j->i as two different edges)\n",
        "\n",
        "    return dataset,data,G\n",
        "def get_dataset_graphs(name: str, data_dir=DEFAULT_DATA_PATH,make_undirected: bool = False):\n",
        "\n",
        "    path = data_dir\n",
        "    if name == \"MUTAG\":\n",
        "        dataset = TUDataset(root=path, name=name)\n",
        "    elif name == \"ENZYMES\":\n",
        "        dataset = TUDataset(root=path, name=name)\n",
        "    elif name == \"PROTEINS\":\n",
        "        dataset = TUDataset(root=path, name=name)\n",
        "    elif name == \"IMDB-BINARY\":\n",
        "        dataset = TUDataset(root=path, name=name)\n",
        "    else:\n",
        "        raise Exception(f\"Unknown dataset: {name}\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "\n",
        "def data_information(dataset,data):\n",
        "    print()\n",
        "    print(f'Dataset: {dataset}:')\n",
        "    print('======================')\n",
        "\n",
        "    print(f'Number of features: {dataset.num_features}')\n",
        "    print(f'Number of classes: {dataset.num_classes}')\n",
        "    print()\n",
        "\n",
        "    # Gather some statistics about the graph.\n",
        "    print(f'Number of nodes: {data.num_nodes}')\n",
        "    print(f'Number of edges: {data.num_edges}')\n",
        "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "    print(f'Has self-loops: {data.has_self_loops()}')\n",
        "    print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZgcU6fZ-i9x"
      },
      "source": [
        "## LargestConnectedCommonent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iVt_DHRc-i9x"
      },
      "outputs": [],
      "source": [
        "def get_largest_connected_component_pytorch(connectiontype: str, directed ,data,num_nodes:int):\n",
        "    \"\"\"\n",
        "    Pytorch (backend Scipy) implementation of lcc calculation\n",
        "\n",
        "    Args:\n",
        "    edge_index (torch tensor): edge index of the graph\n",
        "    num nodes (int): number of nodes of the graph\n",
        "    directed: (boolean): Wether input graph is directed or not.  If directed == False, connectiontype keyword is not referenced.\n",
        "    connectiontype (str): Only relevant for directed graphs. Weak or Strong constrain for largest connected component\n",
        "\n",
        "    Returns:\n",
        "    Subgraph (Networkx Graph): Subgraph which corresponds to the largest connected component of the graph\n",
        "    \"\"\"\n",
        "\n",
        "    adj = torch_geometric.utils.to_scipy_sparse_matrix(data.edge_index, num_nodes=num_nodes)\n",
        "    num_components, component = sparse.csgraph.connected_components(adj, directed=directed, connection=connectiontype)\n",
        "    #print(\"Total Number of Components: \",num_components)\n",
        "\n",
        "    _, count = np.unique(component, return_counts=True)\n",
        "    subset_np = np.in1d(component, count.argsort()[-1:])\n",
        "    subset = torch.from_numpy(subset_np)\n",
        "    subset = subset.to(data.edge_index.device, torch.bool)\n",
        "    Subgraph = torch_geometric.utils.to_networkx(data.subgraph(subset))\n",
        "    #print(\"Largest Connected Component size: \", len(Subgraph.nodes))\n",
        "    return data.subgraph(subset)#Subgraph\n",
        "\n",
        "def get_largest_connected_component_networkx(connectiontype: str,directed, G):\n",
        "    \"\"\"\n",
        "    Network implementation of lcc calculation\n",
        "\n",
        "    Args:\n",
        "    G (networkx graph): Input Graph\n",
        "    connectiontype (str or None): Only relevant for directed graphs. Weak or Strong constrain for largest connected component\n",
        "\n",
        "    Returns:\n",
        "    Subgraph (Networkx Graph): Subgraph which corresponds to the largest connected component of the graph\n",
        "    \"\"\"\n",
        "    if not directed:\n",
        "        return G.subgraph(max(nx.connected_components(G), key=len)).copy()\n",
        "    elif directed and connectiontype == 'strong':\n",
        "        return G.subgraph(max(nx.strongly_connected_components(G), key=len)).copy()\n",
        "    elif directed and connectiontype == 'weak':\n",
        "        return G.subgraph(max(nx.weakly_connected_components(G), key=len)).copy()\n",
        "\n",
        "def get_component_toppingetal(data, start: int = 0) -> set:\n",
        "    visited_nodes = set()\n",
        "    queued_nodes = set([start])\n",
        "    row, col = data.edge_index.numpy()\n",
        "    while queued_nodes:\n",
        "        current_node = queued_nodes.pop()\n",
        "        visited_nodes.update([current_node])\n",
        "        neighbors = col[np.where(row == current_node)[0]]\n",
        "        neighbors = [\n",
        "            n for n in neighbors if n not in visited_nodes and n not in queued_nodes\n",
        "        ]\n",
        "        queued_nodes.update(neighbors)\n",
        "    return visited_nodes\n",
        "\n",
        "def get_largest_connected_component_toppingetal(data):\n",
        "\n",
        "    remaining_nodes = set(range(data.x.shape[0]))\n",
        "    comps = []\n",
        "    while remaining_nodes:\n",
        "        start = min(remaining_nodes)\n",
        "        comp = get_component_toppingetal(data, start)\n",
        "        comps.append(comp)\n",
        "        remaining_nodes = remaining_nodes.difference(comp)\n",
        "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
        "\n",
        "def remap_edges(edges: list, mapper: dict) -> list:\n",
        "    row = [e[0] for e in edges]\n",
        "    col = [e[1] for e in edges]\n",
        "    row = list(map(lambda x: mapper[x], row))\n",
        "    col = list(map(lambda x: mapper[x], col))\n",
        "    return [row, col]\n",
        "\n",
        "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
        "    mapper = {}\n",
        "    counter = 0\n",
        "    for node in lcc:\n",
        "        mapper[node] = counter\n",
        "        counter += 1\n",
        "    return mapper\n",
        "\n",
        "def lcc_dataset(dataset,to_undirected = True):\n",
        "    lcc = get_largest_connected_component_toppingetal(dataset[0])\n",
        "\n",
        "    x_new = dataset.data.x[lcc]\n",
        "    y_new = dataset.data.y[lcc]\n",
        "\n",
        "    row, col = dataset.data.edge_index.numpy()\n",
        "    edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
        "    edges = remap_edges(edges, get_node_mapper(lcc))\n",
        "\n",
        "    if to_undirected:\n",
        "        data = Data(\n",
        "            x=x_new,\n",
        "            edge_index=torch_geometric.utils.to_undirected(torch.LongTensor(edges)),\n",
        "            y=y_new,\n",
        "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "             )\n",
        "    else:\n",
        "        data = Data(\n",
        "            x=x_new,\n",
        "            edge_index=torch.LongTensor(edges),\n",
        "            y=y_new,\n",
        "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
        "             )\n",
        "    dataset.data = data\n",
        "\n",
        "    mapping = dict(\n",
        "        zip(np.unique(dataset.data.y), range(len(np.unique(dataset.data.y))))\n",
        "    )\n",
        "    dataset.data.y = torch.LongTensor([mapping[u] for u in np.array(dataset.data.y)])\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H5t3NEQ-3rB"
      },
      "source": [
        "## GNN Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p1U2xsxS-7GB"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from torch.nn import ModuleList, Dropout, ReLU\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class GCN_Toppingetal(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, dataset, hidden: List[int] = [64], dropout: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        num_features = [dataset.data.x.shape[1]] + hidden + [dataset.num_classes]\n",
        "        layers = []\n",
        "        for in_features, out_features in zip(num_features[:-1], num_features[1:]):\n",
        "            layers.append(GCNConv(in_features, out_features))\n",
        "        self.layers = ModuleList(layers)\n",
        "\n",
        "        self.reg_params = list(layers[0].parameters())\n",
        "        self.non_reg_params = list([p for l in layers[1:] for p in l.parameters()])\n",
        "\n",
        "        self.dropout = Dropout(p=dropout)\n",
        "        self.act_fn = ReLU()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_parameters()\n",
        "\n",
        "    def forward(self, data: Data,device):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        #x = x.to(device = device)\n",
        "        #edge_index = edge_index.to(device = device)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x, edge_index, edge_weight=edge_attr)\n",
        "\n",
        "            if i == len(self.layers) - 1:\n",
        "                break\n",
        "\n",
        "            x = self.act_fn(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        return torch.nn.functional.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features,num_classes,hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(num_features,hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels,num_classes)\n",
        "    def forward(self, data):\n",
        "        x,edge_index = data.x,data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.4144)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GCN_Graph(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, dataset, hidden: List[int] = [64], dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        num_features = [dataset.data.x.shape[1]] + hidden + [dataset.num_classes]\n",
        "        layers = []\n",
        "        for in_features, out_features in zip(num_features[:-1], num_features[1:]):\n",
        "            layers.append(GCNConv(in_features, out_features))\n",
        "        self.layers = ModuleList(layers)\n",
        "        self.num_layers = len(layers)\n",
        "        self.dropout = Dropout(p=dropout)\n",
        "        self.act_fn = ReLU()\n",
        "\n",
        "    def forward(self, graph):\n",
        "        x, edge_index, batch = graph.x, graph.edge_index, graph.batch\n",
        "        x = x.float()\n",
        "        for i, layer in enumerate(self.layers):\n",
        "\n",
        "            x_new = layer(x, edge_index)\n",
        "            if i != self.num_layers - 1:\n",
        "                x_new = self.act_fn(x_new)\n",
        "                x_new = self.dropout(x_new)\n",
        "\n",
        "            x = x_new \n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux0jht5T--DJ"
      },
      "source": [
        "## Experiment Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZH1WSXsY--3B"
      },
      "outputs": [],
      "source": [
        "class Experiment():\n",
        "    def __init__(self,device,datasetname,dataset,data,hyperparameters):\n",
        "\n",
        "        #self.model = GCN(dataset.num_features,dataset.num_classes,hidden_channels=64 )\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "        self.lr = hyperparameters[\"learning_rate\"]\n",
        "        self.layers = hyperparameters[\"layers\"]\n",
        "        self.weight_decay = hyperparameters[\"weight_decay\"]\n",
        "        self.dropout = hyperparameters[\"dropout\"]\n",
        "\n",
        "        self.model = GCN_Toppingetal(dataset,self.layers,self.dropout).to(device = device)\n",
        "        self.device = device\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay= self.weight_decay)\n",
        "\n",
        "        self.epoch = 10000#hyperparameters[\"epochs\"]\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "        out = self.model(self.data,self.device)  # Perform a single forward pass.\n",
        "        loss = F.nll_loss(out[self.data.train_mask], self.data.y[self.data.train_mask].to(self.device))  # Compute the loss solely based on the training nodes.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        self.optimizer.step()  # Update parameters based on gradients.\n",
        "        return loss\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        out = self.model(self.data,self.device)  # Perform a single forward pass.\n",
        "        pred = out[self.data.val_mask].max(1)[1]\n",
        "        val_acc = pred.eq(self.data.y[self.data.val_mask]).sum().item() / self.data.val_mask.sum().item()\n",
        "\n",
        "        return val_acc\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        out = self.model(self.data,self.device)  # Perform a single forward pass.\n",
        "        pred = out[self.data.test_mask].max(1)[1]\n",
        "        test_acc = pred.eq(self.data.y[self.data.test_mask]).sum().item() / self.data.test_mask.sum().item()\n",
        "\n",
        "        return test_acc\n",
        "\n",
        "    def training(self):\n",
        "        losses = []\n",
        "        validations = []\n",
        "        counter = 0\n",
        "        for epoch in range(1, self.epoch):\n",
        "            loss = self.train()\n",
        "            losses.append(loss.detach().cpu().numpy())\n",
        "            val = self.validate()\n",
        "            validations.append(val)\n",
        "            if epoch ==1:\n",
        "                best_val = val\n",
        "            elif epoch > 1 and val >= best_val:\n",
        "                best_val = val\n",
        "                counter = 0\n",
        "            else:\n",
        "                counter += 1\n",
        "\n",
        "            if counter > 100:\n",
        "                #print(\"Early stopping at Epoch: \", epoch)\n",
        "                break\n",
        "        return losses,validations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphExperiment():\n",
        "    def __init__(self,device,dataset,hyperparameters):\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.lr = hyperparameters[\"learning_rate\"]\n",
        "        self.layers = hyperparameters[\"layers\"]\n",
        "        self.weight_decay = hyperparameters[\"weight_decay\"]\n",
        "        self.dropout = hyperparameters[\"dropout\"]\n",
        "\n",
        "        self.model = GCN_Graph(dataset, self.layers,self.dropout).to(device = device)\n",
        "\n",
        "        self.loss_fn =  torch.nn.NLLLoss()\n",
        "        self.softmax =  torch.nn.LogSoftmax(dim=1)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay= self.weight_decay)\n",
        "\n",
        "        self.epoch = 10000#hyperparameters[\"epochs\"]\n",
        "    def train(self,train_loader):\n",
        "\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        self.optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "        for graph in train_loader:\n",
        "            graph = graph.to(\"cuda\")\n",
        "            y = graph.y.to(\"cuda\")\n",
        "\n",
        "            out = self.model(graph)\n",
        "            loss = self.loss_fn(self.softmax(out), y)\n",
        "            total_loss += loss\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "        return total_loss\n",
        "\n",
        "    def eval(self, loader):\n",
        "        \"\"\"\"\n",
        "        Depending on the loader this can be either the validation or the test set\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        sample_size = len(loader.dataset)\n",
        "        with torch.no_grad():\n",
        "            total_correct = 0\n",
        "            for graph in loader:\n",
        "                graph = graph.to(self.device)\n",
        "                y = graph.y.to(self.device)\n",
        "                out = self.model(graph)\n",
        "                _, pred = out.max(dim=1)\n",
        "                total_correct += pred.eq(y).sum().item()\n",
        "                \n",
        "        return total_correct / sample_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp3MfHAL_Gxb"
      },
      "source": [
        "## Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3ANdNfer_HXO"
      },
      "outputs": [],
      "source": [
        "def set_train_val_test_split_frac(seed: int, data: Data, val_frac: float, test_frac: float):\n",
        "    num_nodes = data.y.shape[0]\n",
        "\n",
        "    val_size = ceil(val_frac * num_nodes)\n",
        "    test_size = ceil(test_frac * num_nodes)\n",
        "    train_size = num_nodes - val_size - test_size\n",
        "\n",
        "    nodes = list(range(num_nodes))\n",
        "\n",
        "    # Take same test set every time using development seed for robustness\n",
        "    random.seed(development_seed)\n",
        "    random.shuffle(nodes)\n",
        "    test_idx = sorted(nodes[:test_size])\n",
        "    nodes = [x for x in nodes if x not in test_idx]\n",
        "\n",
        "    # Take train / val split according to seed\n",
        "    random.seed(seed)\n",
        "    random.shuffle(nodes)\n",
        "    train_idx = sorted(nodes[:train_size])\n",
        "    val_idx = sorted(nodes[train_size:])\n",
        "\n",
        "    assert len(train_idx) + len(val_idx) + len(test_idx) == num_nodes\n",
        "\n",
        "    def get_mask(idx):\n",
        "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        mask[idx] = 1\n",
        "        return mask\n",
        "\n",
        "    data.train_mask = get_mask(train_idx)\n",
        "    data.val_mask = get_mask(val_idx)\n",
        "    data.test_mask = get_mask(test_idx)\n",
        "\n",
        "    return data\n",
        "\n",
        "def set_train_val_test_split(\n",
        "        seed: int,\n",
        "        data: Data,\n",
        "        development_frac: float = 0.5,\n",
        "        num_per_class: int = 20) -> Data:\n",
        "    rnd_state = np.random.RandomState(development_seed)\n",
        "    num_nodes = data.y.shape[0]\n",
        "\n",
        "    #num_development = ceil(development_frac * num_nodes)\n",
        "    num_development = 1500 #ceil(development_frac * num_nodes)\n",
        "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
        "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
        "\n",
        "    train_idx = []\n",
        "    rnd_state = np.random.RandomState(seed)\n",
        "    for c in range(data.y.max() + 1):\n",
        "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
        "        train_idx.extend(rnd_state.choice(class_idx, min(num_per_class, ceil(len(class_idx) * 0.5)), replace=False))\n",
        "\n",
        "    val_idx = [i for i in development_idx if i not in train_idx]\n",
        "\n",
        "    def get_mask(idx):\n",
        "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        mask[idx] = 1\n",
        "        return mask\n",
        "\n",
        "    data.train_mask = get_mask(train_idx)\n",
        "    data.val_mask = get_mask(val_idx)\n",
        "    data.test_mask = get_mask(test_idx)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvQ0Xw1S-i9y"
      },
      "source": [
        "## Curvature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9-WpHj9g-i9y"
      },
      "outputs": [],
      "source": [
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], int32[:,:], int32[:,:], float32[:], float32[:], int32, float32[:,:],boolean)\"\n",
        ")\n",
        "\n",
        "def _balanced_forman_curvature_undirected_personal(A, A2,edge_index,indices_neigh,d_in, d_out, N, C,fcc = True):\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    if (i < N) and (j < N):\n",
        "        if A[i, j] == 0:\n",
        "            C[i, j] = 0\n",
        "            return\n",
        "\n",
        "        if d_in[i] > d_out[j]:\n",
        "            d_max = d_in[i]\n",
        "            d_min = d_out[j]\n",
        "        else:\n",
        "            d_max = d_out[j]\n",
        "            d_min = d_in[i]\n",
        "\n",
        "        if d_min == 1:\n",
        "           C[i, j] = 0\n",
        "           return\n",
        "\n",
        "        C[i, j] = ((2 / d_max) + (2 / d_min) - 2\n",
        "                    + (2 / d_max + 1 / d_min) * A2[i, j] * A[i, j]\n",
        "                  )\n",
        "        if fcc:\n",
        "            ind1_i,ind2_i = indices_neigh[i,0], indices_neigh[i,1]\n",
        "            neighs_i = edge_index[1,ind1_i:ind2_i]\n",
        "\n",
        "            ind1_j,ind2_j = indices_neigh[j,0],indices_neigh[j,1]\n",
        "            neighs_j = edge_index[1,ind1_j:ind2_j]\n",
        "\n",
        "\n",
        "            sharp_ij = 0\n",
        "            lambda_ij = 0\n",
        "            for k_count in range(len(neighs_i)):\n",
        "                k = neighs_i[k_count]\n",
        "\n",
        "                ind1_k = indices_neigh[k,0]\n",
        "                ind2_k = indices_neigh[k,1]\n",
        "                neighs_k = edge_index[1,ind1_k:ind2_k]\n",
        "                if A[k,i]*(1-A[k,j]) !=0 and k != j: #Only have k in S(i)\\S(j)\n",
        "\n",
        "                    had = 0\n",
        "                    for l_count in range(len(neighs_k)):\n",
        "                        l = neighs_k[l_count]\n",
        "                        had += A[k,l]*A[i,l]*A[j,l]\n",
        "\n",
        "                    TMP =A[k,i]*(1-A[k,j])*(A2[k,j] -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_ij += 1\n",
        "                        if TMP > lambda_ij:\n",
        "                            lambda_ij = TMP\n",
        "\n",
        "            for k_count in range(len(neighs_j)):\n",
        "                k = neighs_j[k_count]\n",
        "\n",
        "                ind1_k,ind2_k = indices_neigh[k,0],indices_neigh[k,1]\n",
        "                neighs_k = edge_index[1,ind1_k:ind2_k]\n",
        "\n",
        "                if A[j,k]*(1-A[k,i]) !=0 and k != i: #Only have k in S(j)\\S(i)\n",
        "                    had = 0\n",
        "\n",
        "                    for l_count in range(len(neighs_k)):\n",
        "                        l = neighs_k[l_count]\n",
        "                        had += A[k,l]*A[i,l]*A[j,l]\n",
        "\n",
        "                    TMP = A[j,k]*(1-A[k,i])*(A2[k,i] -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_ij += 1\n",
        "                        if TMP > lambda_ij:\n",
        "                            lambda_ij = TMP\n",
        "\n",
        "            if lambda_ij > 0:\n",
        "                C[i, j] += sharp_ij / (d_max * lambda_ij)\n",
        "\n",
        "\n",
        "def balanced_forman_curvature_undirected_personal(A,edge_index, C=None,fcc = True):\n",
        "    N = A.shape[0]\n",
        "    threadsperblock = (32,16)#,10)\n",
        "    blockspergrid_x = math.ceil(N / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(N / threadsperblock[1])\n",
        "\n",
        "    blockspergrid_2d = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "    A2 = torch.matmul(A, A)\n",
        "\n",
        "    d_in = A.sum(axis=0)\n",
        "    d_out = A.sum(axis=1)\n",
        "\n",
        "    ind1 = 0\n",
        "    ind2 = 0\n",
        "    index_tuples = []\n",
        "    for k in range(N):#test:\n",
        "        ind2 += int(d_in[k].item())\n",
        "        index_tuples.append((ind1,ind2))\n",
        "        ind1 = ind2\n",
        "    index_tuples = torch.tensor(index_tuples).cuda()\n",
        "\n",
        "    if C is None:\n",
        "        C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    _balanced_forman_curvature_undirected_personal[blockspergrid_2d, threadsperblock](A, A2,edge_index,index_tuples,d_in, d_out, N, C,fcc)\n",
        "    return C\n",
        "\n",
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], float32[:], float32[:], int32, float32[:,:])\"\n",
        ")\n",
        "def _balanced_forman_curvature_jctopping(A, A2, d_in, d_out, N, C):\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    if (i < N) and (j < N):\n",
        "        if A[i, j] == 0:\n",
        "            C[i, j] = 0\n",
        "            return\n",
        "\n",
        "        if d_in[i] > d_out[j]:\n",
        "            d_max = d_in[i]\n",
        "            d_min = d_out[j]\n",
        "        else:\n",
        "            d_max = d_out[j]\n",
        "            d_min = d_in[i]\n",
        "\n",
        "        if d_max * d_min == 0:\n",
        "            C[i, j] = 0\n",
        "            return\n",
        "\n",
        "        sharp_ij = 0\n",
        "        lambda_ij = 0\n",
        "        for k in range(N):\n",
        "            TMP = A[k, j] * (A2[i, k] - A[i, k]) * A[i, j]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "            TMP = A[i, k] * (A2[k, j] - A[k, j]) * A[i, j]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "        C[i, j] = (\n",
        "            (2 / d_max) + (2 / d_min) - 2 + (2 / d_max + 1 / d_min) * A2[i, j] * A[i, j]\n",
        "        )\n",
        "        if lambda_ij > 0:\n",
        "            C[i, j] += sharp_ij / (d_max * lambda_ij)\n",
        "\n",
        "\n",
        "def balanced_forman_curvature_jctopping(A, C=None):\n",
        "    N = A.shape[0]\n",
        "    A2 = torch.matmul(A, A)\n",
        "    d_in = A.sum(axis=0)\n",
        "    d_out = A.sum(axis=1)\n",
        "    if C is None:\n",
        "        C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    threadsperblock = (16, 16)\n",
        "    blockspergrid_x = math.ceil(N / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(N / threadsperblock[1])\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "    _balanced_forman_curvature_jctopping[blockspergrid, threadsperblock](A, A2, d_in, d_out, N, C)\n",
        "    return C\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcrL8toT-i9z"
      },
      "source": [
        "## SDRF Cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "tDmdEno9-i9z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\numba\\core\\config.py:197: RuntimeWarning: Environment variable 'NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS' is defined but its associated value 'false' could not be parsed.\n",
            "The parse failed with exception: invalid literal for int() with base 10: 'false'.\n",
            "  warnings.warn(f\"Environment variable '{name}' is defined but \"\n"
          ]
        }
      ],
      "source": [
        "NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS=0\n",
        "\n",
        "def softmax(a, tau=1):\n",
        "    exp_a = np.exp(a * tau)\n",
        "    return exp_a / exp_a.sum()\n",
        "\n",
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], int32[:,:], int32[:,:], float32, float32, int32, float32[:,:], int32, int32, int32[:], int32[:], int32, int32,boolean)\"\n",
        ")\n",
        "\n",
        "def _curvature_post_rewiring_undirected_personal( A, A2,edge_index,indices_neigh, d_in_x, d_out_y, N, D, x, y, i_neighbors, j_neighbors, dim_i, dim_j,fcc = True\n",
        "):\n",
        "    I, J = cuda.grid(2)\n",
        "\n",
        "    if (I < dim_i) and (J < dim_j):\n",
        "        i = i_neighbors[I]\n",
        "        j = j_neighbors[J]\n",
        "\n",
        "        if (i == j) or (A[i, j] != 0):\n",
        "            D[I, J] = -1000\n",
        "            return\n",
        "\n",
        "        A_i_j = A[i, j]\n",
        "        A_i_j += 1\n",
        "\n",
        "        if i == x:\n",
        "            d_in_x += 1\n",
        "        elif j == y:\n",
        "            d_out_y += 1\n",
        "\n",
        "\n",
        "        if d_in_x > d_out_y:\n",
        "            d_max = d_in_x\n",
        "            d_min = d_out_y\n",
        "        else:\n",
        "            d_max = d_out_y\n",
        "            d_min = d_in_x\n",
        "\n",
        "        if d_min ==1:#d_in_x * d_out_y == 0:\n",
        "            D[I, J] = 0\n",
        "            return\n",
        "\n",
        "        A2_x_y = A2[x, y]\n",
        "        # Difference in triangles term\n",
        "        if (x == i) and (A[j, y] != 0):\n",
        "            A2_x_y += 1.\n",
        "        elif (y == j) and (A[x, i] != 0):\n",
        "            A2_x_y += 1.\n",
        "\n",
        "        # Difference in four-cycles term\n",
        "        ind1_x,ind2_x = indices_neigh[x,0], indices_neigh[x,1]\n",
        "        neighs_x = edge_index[1,ind1_x:ind2_x]\n",
        "\n",
        "        ind1_y,ind2_y = indices_neigh[y,0],indices_neigh[y,1]\n",
        "        neighs_y = edge_index[1,ind1_y:ind2_y]\n",
        "\n",
        "        D[I, J] = (\n",
        "                (2 / d_max)\n",
        "                + (2 / d_min)\n",
        "                - 2\n",
        "                + (2 / d_max + 1 / d_min) * A2_x_y * A[x, y]\n",
        "            )\n",
        "\n",
        "        if fcc:\n",
        "\n",
        "            sharp_xy = 0\n",
        "            lambda_xy = 0\n",
        "\n",
        "\n",
        "            A_x_j = A[x,j] + 0\n",
        "            if i == x and y !=j:\n",
        "                A_x_j += 1\n",
        "\n",
        "            for k_count in range(len(neighs_x)):\n",
        "                k = neighs_x[k_count]\n",
        "\n",
        "                ind1_k = indices_neigh[k,0]\n",
        "                ind2_k = indices_neigh[k,1]\n",
        "                neighs_k = edge_index[1,ind1_k:ind2_k]\n",
        "\n",
        "                if k != i and k != j and y !=i and y!=j:\n",
        "                    A2_k_y = A2[k, y]\n",
        "                elif k ==i and y !=j:\n",
        "                    A2_k_y = A2[k, y] + A[j,y]\n",
        "                elif k ==j and y !=i:\n",
        "                    A2_k_y = A2[k, y] + A[i,y]\n",
        "                elif k!=j and y==i:\n",
        "                    A2_k_y = A2[k, y] + A[k,j]\n",
        "                elif k!=i and y==j:\n",
        "                    A2_k_y = A2[k, y] + A[k,i]\n",
        "                elif (k ==i and y ==j) or (k ==j and y == i):\n",
        "                    A2_k_y = A2[k, y] + +1*A[k,k] + 1*A[y,y]\n",
        "\n",
        "                A_k_y = A[k,y] + 0\n",
        "                A_x_k = A[x,k] + 0\n",
        "\n",
        "                if  (k == i and j ==y) or (k == y and j == i):\n",
        "                    A_k_y +=1\n",
        "                if (i == x and k == j) or (x==j and k == i):\n",
        "                    A_x_k +=1\n",
        "\n",
        "                if A_x_k*(1-A_k_y) !=0 and k!=y:\n",
        "\n",
        "                    had = 0\n",
        "                    for l_count in range(len(neighs_k)): #This doesn't sum over j since we haven't adapted the edge index yet\n",
        "                        l = neighs_k[l_count]\n",
        "                        A_k_l = A[k,l] + 0\n",
        "                        A_x_l = A[x,l] + 0\n",
        "                        A_y_l = A[y,l] + 0\n",
        "                        if (k == i and l == j) or (k == j and l == i):\n",
        "                            A_k_l +=1\n",
        "                        if (x == i and l == j) or (x == j and l == i):\n",
        "                            A_x_l +=1\n",
        "                        if (y == i and l == j) or (y == j and l == i):\n",
        "                            A_y_l +=1\n",
        "                        had += A_k_l*A_x_l*A_y_l\n",
        "\n",
        "                    TMP =A_x_k*(1-A_k_y)*(A2_k_y -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_xy += 1\n",
        "                        if TMP > lambda_xy:\n",
        "                            lambda_xy = TMP\n",
        "\n",
        "            for w_count in range(len(neighs_y)):\n",
        "                w = neighs_y[w_count]\n",
        "                ind1_w = indices_neigh[w,0]\n",
        "                ind2_w = indices_neigh[w,1]\n",
        "                neighs_w = edge_index[1,ind1_w:ind2_w]\n",
        "\n",
        "                if w != i and w != j and x !=i and x!=j:\n",
        "                    A2_w_x = A2[w, x]\n",
        "                elif w ==i and x !=j:\n",
        "                    A2_w_x = A2[w, x] + A[j,x]\n",
        "                elif w ==j and x !=i:\n",
        "                    A2_w_x = A2[w, x] + A[i,x]\n",
        "                elif w!=j and x==i:\n",
        "                    A2_w_x = A2[w, x] + A[w,j]\n",
        "                elif w!=i and x==j:\n",
        "                    A2_w_x = A2[w, x] + A[w,i]\n",
        "                elif (w ==i and x ==j) or (w ==j and x == i):\n",
        "                    A2_w_x = A2[w, x] +1*A[w,w] + 1*A[x,x]\n",
        "\n",
        "                A_x_w = A[x,w] + 0\n",
        "                if  w ==j and x ==i:\n",
        "                    A_x_w +=1\n",
        "\n",
        "                A_y_w = A[y,w] + 0\n",
        "                A_w_x = A[w,x] + 0\n",
        "\n",
        "                if  (w == i and j ==y) or (w == y and j == i):\n",
        "                        A_y_w +=1\n",
        "                if (i == x and w == j) or (x==j and w == i):\n",
        "                        A_w_x +=1\n",
        "\n",
        "                if A_y_w*(1-A_w_x) !=0 and w != x:\n",
        "                    had = 0\n",
        "                    for l_count in range(len(neighs_w)): # If w ==j (SHOULD NEVER HAPPEN), this doesn't sum over i since we haven't adapted the edge index yet\n",
        "                        l = neighs_w[l_count]\n",
        "                        A_w_l = A[w,l] + 0\n",
        "                        A_x_l = A[x,l] + 0\n",
        "                        A_y_l = A[y,l] + 0\n",
        "                        if (w == i and l == j) or (w == j and l == i):\n",
        "                            A_w_l +=1\n",
        "                        if (x == i and l == j) or (x == j and l == i):\n",
        "                            A_x_l +=1\n",
        "                        if (y == i and l == j) or (y == j and l == i):\n",
        "                            A_y_l +=1\n",
        "\n",
        "                        had += A_w_l*A_x_l*A_y_l\n",
        "\n",
        "\n",
        "                    TMP = A_y_w*(1-A_x_w)*(A2_w_x -had- 1)\n",
        "\n",
        "                    if TMP > 0:\n",
        "                        sharp_xy +=  1\n",
        "                        if TMP > lambda_xy:\n",
        "                            lambda_xy = TMP\n",
        "\n",
        "\n",
        "            if lambda_xy > 0:\n",
        "                D[I, J] += sharp_xy / (d_max * lambda_xy)\n",
        "\n",
        "\n",
        "def curvature_post_rewiring_personal(A, x, y,edge_index, i_neighbors, j_neighbors, D=None,is_undirected = False,fcc = True):\n",
        "\n",
        "    N = A.shape[0]\n",
        "    A2 = torch.matmul(A, A)\n",
        "    d_in = A.sum(axis = 0)#A[:, x].sum()\n",
        "    d_out = A.sum(axis = 1)#A[y].sum()\n",
        "    if D is None:\n",
        "        D = torch.zeros(len(i_neighbors), len(j_neighbors)).cuda()\n",
        "\n",
        "    ind1 = 0\n",
        "    ind2 = 0\n",
        "    index_tuples = []\n",
        "    for k in range(N):\n",
        "        ind2 += int(d_in[k].item())\n",
        "        index_tuples.append((ind1,ind2))\n",
        "        ind1 = ind2\n",
        "    index_tuples = torch.tensor(index_tuples).cuda()\n",
        "\n",
        "    d_in = d_in[x]\n",
        "    d_out = d_out[y]\n",
        "    threadsperblock = (16, 16)\n",
        "    blockspergrid_x = math.ceil(D.shape[0] / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(D.shape[1] / threadsperblock[1])\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "    if is_undirected:\n",
        "        _curvature_post_rewiring_undirected_personal[blockspergrid, threadsperblock](\n",
        "            A,\n",
        "            A2,\n",
        "            edge_index,\n",
        "            index_tuples,\n",
        "            d_in,\n",
        "            d_out,\n",
        "            N,\n",
        "            D,\n",
        "            x,\n",
        "            y,\n",
        "            np.array(i_neighbors),\n",
        "            np.array(j_neighbors),\n",
        "            D.shape[0],\n",
        "            D.shape[1],\n",
        "            fcc\n",
        "        )\n",
        "    else:\n",
        "        print(\"Not implemented for directed graphs\")\n",
        "        return\n",
        "    return D\n",
        "\n",
        "\n",
        "def sdrf_cuda_personal(\n",
        "    data,\n",
        "    loops=10,\n",
        "    remove_edges=False,\n",
        "    removal_bound=0.5,\n",
        "    tau=1,\n",
        "    int_node = False,\n",
        "    is_undirected=False,\n",
        "    fcc = True\n",
        "):\n",
        "    N = data.num_nodes\n",
        "    G_in = torch_geometric.utils.to_networkx(data)\n",
        "\n",
        "    if is_undirected:\n",
        "        G_in = G_in.to_undirected()\n",
        "\n",
        "    #print(\"Start\", G_in)\n",
        "    count_edge_removal = 0\n",
        "\n",
        "    A = torch.tensor(nx.adjacency_matrix(G_in).todense(), dtype = torch.float)\n",
        "    A = A.cuda()\n",
        "\n",
        "\n",
        "    edge_index = data.edge_index.clone()\n",
        "    edge_index = edge_index.cuda()\n",
        "    N = A.shape[0]\n",
        "\n",
        "    C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    for idx in range(loops):\n",
        "\n",
        "        count_new_node = len(G_in.nodes)\n",
        "        can_add = True\n",
        "        if is_undirected:\n",
        "            balanced_forman_curvature_undirected_personal(A,edge_index ,C=C,fcc = fcc)\n",
        "        else:\n",
        "            print(\"Not implemented for directed graphs\")\n",
        "            return\n",
        "\n",
        "\n",
        "        ix_min = C.argmin()\n",
        "\n",
        "\n",
        "        x =  torch.div(ix_min,N,rounding_mode='trunc')\n",
        "        y = ix_min % N\n",
        "\n",
        "        x = x.item()\n",
        "        y = y.item()\n",
        "\n",
        "\n",
        "        if is_undirected:\n",
        "            x_neighbors = list(G_in.neighbors(x)) + [x] # !! We're adding x to the set of neighbours\n",
        "            y_neighbors = list(G_in.neighbors(y)) + [y]\n",
        "        else:\n",
        "            x_neighbors = list(G_in.successors(x)) + [x]\n",
        "            y_neighbors = list(G_in.predecessors(y)) + [y]\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "\n",
        "        for i in x_neighbors:\n",
        "            for j in y_neighbors:\n",
        "                if (i != j) and (not G_in.has_edge(i, j)):\n",
        "                    candidates.append((i, j))\n",
        "\n",
        "        if len(candidates):\n",
        "            D = curvature_post_rewiring_personal(A,x,y,edge_index,x_neighbors,y_neighbors,D=None,is_undirected=is_undirected,fcc = fcc)\n",
        "            improvements = []\n",
        "            for (i, j) in candidates:\n",
        "                improvements.append(\n",
        "                    (D-C[x,y])[x_neighbors.index(i), y_neighbors.index(j)].item()\n",
        "                )\n",
        "            k, l = candidates[np.random.choice(range(len(candidates)), p=softmax(np.array(improvements), tau=tau))] ##For directed graph: Makes sense: k is selected uit of \"i\" and \"l\" out of j\n",
        "\n",
        "            if int_node:\n",
        "                A = F.pad(input=A, pad=(0,1,0,1), mode='constant', value=0)\n",
        "                G_in.add_node(count_new_node)\n",
        "                G_in.add_edge(k,count_new_node)\n",
        "                G_in.add_edge(count_new_node, l)\n",
        "                if is_undirected:\n",
        "                    A[k, count_new_node] = A[count_new_node, l] = 1.\n",
        "                    A[count_new_node, k] = A[l, count_new_node] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                else:\n",
        "                    A[k, count_new_node] = A[count_new_node, l] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "            else:\n",
        "                G_in.add_edge(k, l)\n",
        "                if is_undirected:\n",
        "                    A[k, l] = A[l, k] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                else:\n",
        "                    A[k, l] = 1.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "\n",
        "        else:\n",
        "            can_add = False\n",
        "            if not remove_edges:\n",
        "                break\n",
        "\n",
        "        if remove_edges:\n",
        "            ix_max = C.argmax()\n",
        "            xmax = torch.div(ix_max,N,rounding_mode='trunc').item()\n",
        "            ymax = (ix_max % N).item()\n",
        "            if C[xmax, ymax] > removal_bound:\n",
        "                G_in.remove_edge(xmax, ymax)\n",
        "\n",
        "                if is_undirected:\n",
        "                    A[xmax, ymax] = A[ymax, xmax] = 0.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                else:\n",
        "                    A[xmax, ymax] = 0.\n",
        "                    edge_index=A.to_sparse().indices()\n",
        "                count_edge_removal += 1\n",
        "\n",
        "            else:\n",
        "                if can_add is False:\n",
        "                    break\n",
        "\n",
        "        #Dcomputed = D[x_neighbors.index(k), y_neighbors.index(l)].item()\n",
        "        #Cnew = balanced_forman_curvature_undirected_personal(A,edge_index ,fcc = fcc)\n",
        "        #print(Cnew[x,y])\n",
        "        #print(Dcomputed)\n",
        "    return G_in,count_edge_removal\n",
        "\n",
        "@cuda.jit(\n",
        "    \"void(float32[:,:], float32[:,:], float32, float32, int32, float32[:,:], int32, int32, int32[:], int32[:], int32, int32)\"\n",
        ")\n",
        "def _balanced_forman_post_delta_jctopping(\n",
        "    A, A2, d_in_x, d_out_y, N, D, x, y, i_neighbors, j_neighbors, dim_i, dim_j\n",
        "):\n",
        "    I, J = cuda.grid(2)\n",
        "\n",
        "    if (I < dim_i) and (J < dim_j):\n",
        "        i = i_neighbors[I]\n",
        "        j = j_neighbors[J]\n",
        "\n",
        "        if (i == j) or (A[i, j] != 0):\n",
        "            D[I, J] = -1000\n",
        "            return\n",
        "\n",
        "        # Difference in degree terms\n",
        "        if j == x:\n",
        "            d_in_x += 1\n",
        "        elif i == y:\n",
        "            d_out_y += 1\n",
        "\n",
        "        if d_in_x * d_out_y == 0:\n",
        "            D[I, J] = 0\n",
        "            return\n",
        "\n",
        "        if d_in_x > d_out_y:\n",
        "            d_max = d_in_x\n",
        "            d_min = d_out_y\n",
        "        else:\n",
        "            d_max = d_out_y\n",
        "            d_min = d_in_x\n",
        "\n",
        "        # Difference in triangles term\n",
        "        A2_x_y = A2[x, y]\n",
        "        if (x == i) and (A[j, y] != 0):\n",
        "            A2_x_y += A[j, y]\n",
        "        elif (y == j) and (A[x, i] != 0):\n",
        "            A2_x_y += A[x, i]\n",
        "\n",
        "        # Difference in four-cycles term\n",
        "        sharp_ij = 0\n",
        "        lambda_ij = 0\n",
        "        for z in range(N):\n",
        "            A_z_y = A[z, y] + 0\n",
        "            A_x_z = A[x, z] + 0\n",
        "            A2_z_y = A2[z, y] + 0\n",
        "            A2_x_z = A2[x, z] + 0\n",
        "\n",
        "            if (z == i) and (y == j):\n",
        "                A_z_y += 1\n",
        "            if (x == i) and (z == j):\n",
        "                A_x_z += 1\n",
        "            if (z == i) and (A[j, y] != 0):\n",
        "                A2_z_y += A[j, y]\n",
        "            if (x == i) and (A[j, z] != 0):\n",
        "                A2_x_z += A[j, z]\n",
        "            if (y == j) and (A[z, i] != 0):\n",
        "                A2_z_y += A[z, i]\n",
        "            if (z == j) and (A[x, i] != 0):\n",
        "                A2_x_z += A[x, i]\n",
        "\n",
        "            TMP = A_z_y * (A2_x_z - A_x_z) * A[x, y]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "            TMP = A_x_z * (A2_z_y - A_z_y) * A[x, y]\n",
        "            if TMP > 0:\n",
        "                sharp_ij += 1\n",
        "                if TMP > lambda_ij:\n",
        "                    lambda_ij = TMP\n",
        "\n",
        "        D[I, J] = (\n",
        "            (2 / d_max) + (2 / d_min) - 2 + (2 / d_max + 1 / d_min) * A2_x_y * A[x, y]\n",
        "        )\n",
        "        if lambda_ij > 0:\n",
        "            D[I, J] += sharp_ij / (d_max * lambda_ij)\n",
        "\n",
        "\n",
        "def balanced_forman_post_delta_jctopping(A, x, y, i_neighbors, j_neighbors, D=None):\n",
        "    N = A.shape[0]\n",
        "    A2 = torch.matmul(A, A)\n",
        "    d_in = A[:, x].sum()\n",
        "    d_out = A[y].sum()\n",
        "    if D is None:\n",
        "        D = torch.zeros(len(i_neighbors), len(j_neighbors)).cuda()\n",
        "\n",
        "    threadsperblock = (16, 16)\n",
        "    blockspergrid_x = math.ceil(D.shape[0] / threadsperblock[0])\n",
        "    blockspergrid_y = math.ceil(D.shape[1] / threadsperblock[1])\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "    _balanced_forman_post_delta_jctopping[blockspergrid, threadsperblock](\n",
        "        A,\n",
        "        A2,\n",
        "        d_in,\n",
        "        d_out,\n",
        "        N,\n",
        "        D,\n",
        "        x,\n",
        "        y,\n",
        "        np.array(i_neighbors),\n",
        "        np.array(j_neighbors),\n",
        "        D.shape[0],\n",
        "        D.shape[1],\n",
        "    )\n",
        "    return D\n",
        "\n",
        "\n",
        "def sdrf_jctopping(\n",
        "    data,\n",
        "    loops=10,\n",
        "    remove_edges=True,\n",
        "    removal_bound=0.5,\n",
        "    tau=1,\n",
        "    is_undirected=False,\n",
        "):\n",
        "    edge_index = data.edge_index\n",
        "    if is_undirected:\n",
        "        edge_index = torch_geometric.utils.to_undirected(edge_index)\n",
        "    A = torch_geometric.utils.to_dense_adj(torch_geometric.utils.remove_self_loops(edge_index)[0])[0]\n",
        "    N = A.shape[0]\n",
        "    G = torch_geometric.utils.to_networkx(data)\n",
        "    if is_undirected:\n",
        "        G = G.to_undirected()\n",
        "    A = A.cuda()\n",
        "    C = torch.zeros(N, N).cuda()\n",
        "\n",
        "    for x in range(loops):\n",
        "        can_add = True\n",
        "        balanced_forman_curvature_jctopping(A, C=C)\n",
        "        ix_min = C.argmin().item()\n",
        "        x = ix_min // N\n",
        "        y = ix_min % N\n",
        "\n",
        "        if is_undirected:\n",
        "            x_neighbors = list(G.neighbors(x)) + [x]\n",
        "            y_neighbors = list(G.neighbors(y)) + [y]\n",
        "        else:\n",
        "            x_neighbors = list(G.successors(x)) + [x]\n",
        "            y_neighbors = list(G.predecessors(y)) + [y]\n",
        "        candidates = []\n",
        "        for i in x_neighbors:\n",
        "            for j in y_neighbors:\n",
        "                if (i != j) and (not G.has_edge(i, j)):\n",
        "                    candidates.append((i, j))\n",
        "\n",
        "        if len(candidates):\n",
        "            D = balanced_forman_post_delta_jctopping(A, x, y, x_neighbors, y_neighbors)\n",
        "            improvements = []\n",
        "            for (i, j) in candidates:\n",
        "                improvements.append(\n",
        "                    (D - C[x, y])[x_neighbors.index(i), y_neighbors.index(j)].item()\n",
        "                )\n",
        "\n",
        "            k, l = candidates[\n",
        "                np.random.choice(\n",
        "                    range(len(candidates)), p=softmax(np.array(improvements), tau=tau)\n",
        "                )\n",
        "            ]\n",
        "            G.add_edge(k, l)\n",
        "            if is_undirected:\n",
        "                A[k, l] = A[l, k] = 1\n",
        "            else:\n",
        "                A[k, l] = 1\n",
        "        else:\n",
        "            can_add = False\n",
        "            if not remove_edges:\n",
        "                break\n",
        "\n",
        "        if remove_edges:\n",
        "            ix_max = C.argmax().item()\n",
        "            x = ix_max // N\n",
        "            y = ix_max % N\n",
        "            if C[x, y] > removal_bound:\n",
        "                G.remove_edge(x, y)\n",
        "                if is_undirected:\n",
        "                    A[x, y] = A[y, x] = 0\n",
        "                else:\n",
        "                    A[x, y] = 0\n",
        "            else:\n",
        "                if can_add is False:\n",
        "                    break\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RBd3V8U-i90"
      },
      "source": [
        "# Experiments Rewiring Sweeps: Undirected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g53Q-agW-i91",
        "outputId": "a4b1d080-9f5d-4b1a-d426-755c8b5ea26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===Starting Rewiring===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/188 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "c:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "100%|██████████| 188/188 [00:50<00:00,  3.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " == Starting Runs == \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]c:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "5it [00:31,  6.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Test accuracy: 53.85\n",
            "--- 81.96891736984253 seconds ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Parameters for the experiment\n",
        "\"\"\"\n",
        "\n",
        "datasetname = \"MUTAG\"\n",
        "batch_size = 64\n",
        "results_dir = \"results\"\n",
        "rewiring_run = True\n",
        "make_undirected = True\n",
        "int_node = False\n",
        "Curvature_type = \"JcT\"\n",
        "\n",
        "path = \"\"\n",
        "\n",
        "train_fraction = 0.6\n",
        "validation_fraction = 0.2\n",
        "\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "os.environ[\"NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS\"] = \"false\"\n",
        "\n",
        "\n",
        "dataset_graphs = get_dataset_graphs(datasetname)\n",
        "\n",
        "sweep_configuration = hyperparameters_Neurips_2[datasetname]\n",
        "\n",
        "sweep_configuration[\"name\"] = datasetname + '_' + Curvature_type\n",
        "\n",
        "from torch_geometric.transforms import BaseTransform\n",
        "\n",
        "class edge_rewiring_transform(BaseTransform):\n",
        "    def __init__(self,hyperparameters,intermediate_node,remove_edges,curvaturetype: str):\n",
        "        \n",
        "        self.hyperparameters= hyperparameters  # parameters you need\n",
        "        self.intermediate_node = intermediate_node\n",
        "        self.remove_edges = remove_edges\n",
        "        self.curvaturetype = curvaturetype\n",
        "\n",
        "    def __call__(self, data: Data) -> Data:\n",
        "        data.edge_index = self.create_rewired_edge_index(data)\n",
        "        return data\n",
        "\n",
        "    def create_rewired_edge_index(self,data : Data):\n",
        "        if self.curvaturetype == \"BFC_w4cycle\":\n",
        "            G_rewired,_ = sdrf_cuda_personal(\n",
        "            data,\n",
        "            loops=self.hyperparameters[\"loops\"],\n",
        "            remove_edges=self.remove_edges,\n",
        "            removal_bound=self.hyperparameters[\"C+\"],\n",
        "            tau=self.hyperparameters[\"tau\"],\n",
        "            int_node = self.intermediate_node,\n",
        "            is_undirected=data.is_undirected(),\n",
        "            fcc = True\n",
        "                        )\n",
        "        elif self.curvaturetype == \"BFC_no4cycle\":\n",
        "            G_rewired,_ = sdrf_cuda_personal(\n",
        "            data,\n",
        "            loops=self.hyperparameters[\"loops\"],\n",
        "            remove_edges=self.remove_edges,\n",
        "            removal_bound=self.hyperparameters[\"C+\"],\n",
        "            tau=self.hyperparameters[\"tau\"],\n",
        "            int_node = self.intermediate_node,\n",
        "            is_undirected=data.is_undirected(),\n",
        "            fcc = False\n",
        "                        )\n",
        "        elif self.curvaturetype == \"JcT\":\n",
        "            G_rewired = sdrf_jctopping(\n",
        "            data,\n",
        "            loops=self.hyperparameters[\"loops\"],\n",
        "            remove_edges=self.remove_edges,\n",
        "            removal_bound=self.hyperparameters[\"C+\"],\n",
        "            tau=self.hyperparameters[\"tau\"],\n",
        "            is_undirected=data.is_undirected(),\n",
        "                        )\n",
        "\n",
        "        edge_index_rewired = torch_geometric.utils.to_undirected(torch.tensor(list(G_rewired.edges)).t())\n",
        "        return edge_index_rewired\n",
        "    \n",
        "def objective(config,dataset,rewire = False):\n",
        "    accuracies = []\n",
        "    test_acc = []\n",
        "    if rewire:\n",
        "        print(\"===Starting Rewiring===\")\n",
        "        \n",
        "        dataset_rew = {}\n",
        "        data_rew = []\n",
        "        transform = torch_geometric.transforms.Compose([edge_rewiring_transform(config_hyper,False,True,\"JcT\")])\n",
        "        for k in tqdm(range(len(dataset))):\n",
        "            data_rew.append(transform(dataset[k]))\n",
        "        dataset_rew[\"data\"] = data_rew\n",
        "        dataset_rew[\"num_classes\"] =  dataset.num_classes\n",
        "        print(\" \")\n",
        "    else:\n",
        "        dataset_rew = dataset\n",
        "    print(\" == Starting Runs == \")\n",
        "    for idx_k,k in tqdm(enumerate(val_seeds[:5])):\n",
        "\n",
        "        dataset_size = len(dataset)\n",
        "        train_size = int(train_fraction * dataset_size)\n",
        "        validation_size = int(validation_fraction * dataset_size)\n",
        "        test_size = dataset_size - train_size - validation_size\n",
        "        development_dataset, test_dataset = random_split(dataset,[train_size + validation_size, test_size], generator=torch.Generator().manual_seed(development_seed)) \n",
        "        train_dataset, validation_dataset = random_split(development_dataset,[train_size,validation_size], generator=torch.Generator().manual_seed(k)) \n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        \n",
        "        Exp = GraphExperiment(device,dataset,config)\n",
        "        counter = 0\n",
        "        for epoch in range(1, Exp.epoch):\n",
        "            loss = Exp.train(train_loader)\n",
        "            \n",
        "            val = Exp.eval(validation_loader)\n",
        "            #wandb.log({\"loss \" + str(idx_k): loss, \"val \" + str(idx_k): val,\"epoch\": epoch})\n",
        "            if epoch ==1:\n",
        "                best_val = val\n",
        "            elif epoch > 1 and val > best_val:\n",
        "                best_val = val\n",
        "                counter = 0\n",
        "            else:\n",
        "                counter += 1\n",
        "            if counter > 100:\n",
        "                break\n",
        "        final_accuracy = Exp.eval(validation_loader)\n",
        "        final_test_acc =  Exp.eval(test_loader)\n",
        "        accuracies.append(final_accuracy)\n",
        "        test_acc.append(final_test_acc)\n",
        "    print(\"\")\n",
        "    return np.mean(np.array(accuracies)),np.mean(np.array(test_acc))\n",
        "\n",
        "#def main():\n",
        "#    wandb.init(dir = \"\")\n",
        "#    acc,test_acc = objective(wandb.config,rewiring_run)\n",
        "#    wandb.log({\"mean accuracy\": acc, \"mean test accuracy\": test_acc})\n",
        "\n",
        "#sweep_id = \"epj1z76g\" # wandb.sweep(sweep=sweep_configuration, project=\"curvature\")\n",
        "#wandb.agent(sweep_id, project=\"Curvature_Neurips\", function=main,count = 50)\n",
        "\n",
        "#sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"Curvature_Neurips\")\n",
        "#wandb.agent(sweep_id, function=main,count = 2)\n",
        "\n",
        "config_hyper = {\n",
        "        \"learning_rate\": 0.07468,\n",
        "        \"layers\": [128],\n",
        "        \"dropout\":0.214,\n",
        "        \"weight_decay\":0.7837,\n",
        "        \"loops\": 20,\n",
        "        \"C+\": 16.881,\n",
        "        \"tau\": 212\n",
        "    } \n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "accuracies,test_accuracies = objective(config_hyper,dataset_graphs,rewiring_run)\n",
        "print(f\"Average Test accuracy: {test_accuracies*100:.2f}\")\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
